# AgenticSeek: Uma Alternativa Privada e Local ao Manus

<p align="center">
<img align="center" src="./media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

  English | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | [Fran√ßais](./README_FR.md) | [Êó•Êú¨Ë™û](./README_JP.md) | [Portugu√™s (Brasil)](./README_PTBR.md) | [Espa√±ol](./README_ES.md)

*Um assistente de IA com reconhecimento de voz que √© uma **alternativa 100% local ao Manus AI**, navega autonomamente na web, escreve c√≥digo e planeja tarefas enquanto mant√©m todos os dados no seu dispositivo. Projetado para modelos de racioc√≠nio local, funciona inteiramente no seu hardware, garantindo total privacidade e zero depend√™ncia de nuvem.*

[![Visitar AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Por que escolher o AgenticSeek?

* üîí Totalmente Local & Privado - Tudo funciona na sua m√°quina, sem nuvem, sem compartilhamento de dados. Seus arquivos, conversas e pesquisas permanecem privados.

* üåê Navega√ß√£o Web Inteligente - O AgenticSeek pode navegar na Internet autonomamente: pesquisar, ler, extrair informa√ß√µes, preencher formul√°rios web, tudo sem interven√ß√£o manual.

* üíª Assistente de Programa√ß√£o Aut√¥nomo - Precisa de c√≥digo? Ele pode escrever, depurar e executar programas em Python, C, Go, Java e muito mais, sem supervis√£o.

* üß† Sele√ß√£o Inteligente de Agentes - Voc√™ pergunta, ele escolhe automaticamente o melhor agente para a tarefa. Como ter uma equipe de especialistas sempre dispon√≠vel.

* üìã Planeja e Executa Tarefas Complexas - Desde o planejamento de viagens at√© projetos complexos, ele pode decompor grandes tarefas em etapas e complet√°-las usando m√∫ltiplos agentes de IA.

* üéôÔ∏è Suporte de Voz - Voz clara, r√°pida e futurista com reconhecimento de voz, permitindo que voc√™ converse como com sua IA pessoal de filme de fic√ß√£o cient√≠fica. (Em desenvolvimento)

### **Demo**

> *Voc√™ pode pesquisar o projeto agenticSeek, aprender quais habilidades s√£o necess√°rias e, em seguida, abrir CV_candidates.zip e me dizer quais correspondem melhor ao projeto?*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Aviso: Esta demonstra√ß√£o e todos os arquivos que aparecem (ex: CV_candidates.zip) s√£o totalmente fict√≠cios. N√£o somos uma empresa, estamos procurando contribuidores de c√≥digo aberto, n√£o candidatos.

> üõ†‚ö†Ô∏èÔ∏è **Trabalho Ativo em Andamento**

> üôè Este projeto come√ßou como um projeto paralelo e n√£o tem roadmap nem financiamento. Cresceu muito al√©m das expectativas ao aparecer no GitHub Trending. Contribui√ß√µes, coment√°rios e paci√™ncia s√£o profundamente apreciados.

## Pr√©-requisitos

Antes de come√ßar, certifique-se de ter instalado:

*   **Git:** Para clonar o reposit√≥rio. [Baixar Git](https://git-scm.com/downloads)
*   **Python 3.10.x:** Python 3.10.x √© altamente recomendado. Outras vers√µes podem causar erros de depend√™ncia. [Baixar Python 3.10](https://www.python.org/downloads/release/python-3100/) (selecione a vers√£o 3.10.x).
*   **Docker Engine & Docker Compose:** Para executar servi√ßos empacotados como SearxNG.
    *   Instalar Docker Desktop (inclui Docker Compose V2): [Windows](https://docs.docker.com/desktop/install/windows-install/) | [Mac](https://docs.docker.com/desktop/install/mac-install/) | [Linux](https://docs.docker.com/desktop/install/linux-install/)
    *   Ou instalar Docker Engine e Docker Compose separadamente no Linux: [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/) (certifique-se de instalar Compose V2, por exemplo `sudo apt-get install docker-compose-plugin`).

### 1. **Clonar o reposit√≥rio e configurar**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Modificar o conte√∫do do arquivo .env

```sh
SEARXNG_BASE_URL="http://searxng:8080" # Se voc√™ executar no modo CLI no host, use http://127.0.0.1:8080
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
```

Atualize o arquivo `.env` conforme necess√°rio:

- **SEARXNG_BASE_URL**: Mantenha inalterado, a menos que voc√™ execute no modo CLI no host.
- **REDIS_BASE_URL**: Mantenha inalterado 
- **WORK_DIR**: Caminho para o diret√≥rio de trabalho local. O AgenticSeek poder√° ler e interagir com esses arquivos.
- **OLLAMA_PORT**: N√∫mero da porta para o servi√ßo Ollama.
- **LM_STUDIO_PORT**: N√∫mero da porta para o servi√ßo LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Porta para qualquer servi√ßo LLM personalizado adicional.

**As chaves de API s√£o completamente opcionais para aqueles que optam por executar LLM localmente, que √© o objetivo principal deste projeto. Deixe-as vazias se voc√™ tiver hardware suficiente.**

### 3. **Iniciar o Docker**

Certifique-se de que o Docker est√° instalado e funcionando no seu sistema. Voc√™ pode iniciar o Docker com os seguintes comandos:

- **Linux/macOS:**  
    Abra um terminal e execute:
    ```sh
    sudo systemctl start docker
    ```
    Ou inicie o Docker Desktop a partir do menu de aplicativos, se instalado.

- **Windows:**  
    Inicie o Docker Desktop a partir do menu Iniciar.

Voc√™ pode verificar se o Docker est√° funcionando executando:
```sh
docker info
```
Se voc√™ vir informa√ß√µes sobre sua instala√ß√£o do Docker, ele est√° funcionando corretamente.

Consulte a [Lista de provedores locais](#lista-de-provedores-locais) abaixo para um resumo.

Pr√≥xima etapa: [Executar o AgenticSeek localmente](#iniciar-os-servi√ßos-e-executar)

*Se voc√™ encontrar problemas, consulte a se√ß√£o [Solu√ß√£o de problemas](#solu√ß√£o-de-problemas).*
*Se seu hardware n√£o puder executar LLM localmente, consulte [Configura√ß√£o para executar com uma API](#configura√ß√£o-para-executar-com-uma-api).*
*Para explica√ß√µes detalhadas do `config.ini`, consulte a [se√ß√£o Configura√ß√£o](#configura√ß√£o).*

---

## Configura√ß√£o para executar LLM localmente na sua m√°quina

**Requisitos de hardware:**

Para executar LLM localmente, voc√™ precisar√° de hardware suficiente. No m√≠nimo, uma GPU capaz de executar Magistral, Qwen ou Deepseek 14B √© necess√°ria. Consulte o FAQ para recomenda√ß√µes detalhadas de modelo/desempenho.

**Configure seu provedor local**  

Inicie seu provedor local, por exemplo com ollama:

```sh
ollama serve
```

Consulte a lista de provedores locais suportados abaixo.

**Atualizar config.ini**

Altere o arquivo config.ini para definir provider_name como um provedor suportado e provider_model como um LLM suportado pelo seu provedor. Recomendamos modelos de racioc√≠nio como *Magistral* ou *Deepseek*.

Consulte o **FAQ** no final do README para o hardware necess√°rio.

```sh
[MAIN]
is_local = True # Se voc√™ est√° executando localmente ou com um provedor remoto.
provider_name = ollama # ou lm-studio, openai, etc.
provider_model = deepseek-r1:14b # escolha um modelo compat√≠vel com seu hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # o nome da sua IA
recover_last_session = True # recuperar a sess√£o anterior
save_session = True # memorizar a sess√£o atual
speak = False # texto para fala
listen = False # fala para texto, apenas para CLI, experimental
jarvis_personality = False # usar uma personalidade mais "Jarvis" (experimental)
languages = en zh # Lista de idiomas, TTS usar√° o primeiro da lista por padr√£o
[BROWSER]
headless_browser = True # mantenha inalterado, a menos que use CLI no host.
stealth_mode = True # Use selenium indetect√°vel para reduzir a detec√ß√£o do navegador
```

**Aviso**:

- O formato do arquivo `config.ini` n√£o suporta coment√°rios.
N√£o copie e cole diretamente a configura√ß√£o de exemplo, pois os coment√°rios causar√£o erros. Em vez disso, modifique manualmente o arquivo `config.ini` com sua configura√ß√£o desejada, sem coment√°rios.

- *N√ÉO* defina provider_name como `openai` se voc√™ estiver usando LM-studio para executar LLM. Use-o como `lm-studio`.

- Alguns provedores (ex: lm-studio) exigem `http://` antes do IP. Exemplo: `http://127.0.0.1:1234`

**Lista de provedores locais**

| Provedor  | Local ? | Descri√ß√£o                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Sim    | Executa LLM localmente facilmente usando ollama |
| lm-studio  | Sim    | Executa LLM localmente com LM studio (defina `provider_name` = `lm-studio`)|
| openai    | Sim     |  Use uma API compat√≠vel com openai (ex: servidor llama.cpp)  |

Pr√≥xima etapa: [Iniciar os servi√ßos e executar o AgenticSeek](#iniciar-os-servi√ßos-e-executar)  

*Se voc√™ encontrar problemas, consulte a se√ß√£o [Solu√ß√£o de problemas](#solu√ß√£o-de-problemas).*
*Se seu hardware n√£o puder executar LLM localmente, consulte [Configura√ß√£o para executar com uma API](#configura√ß√£o-para-executar-com-uma-api).*
*Para explica√ß√µes detalhadas do `config.ini`, consulte a [se√ß√£o Configura√ß√£o](#configura√ß√£o).*

## Configura√ß√£o para executar com uma API

Esta configura√ß√£o usa provedores de LLM externos baseados em nuvem. Voc√™ precisar√° obter chaves de API do servi√ßo escolhido.

**1. Escolha um provedor de API e obtenha uma chave de API:**

Consulte a [Lista de provedores de API](#lista-de-provedores-de-api) abaixo. Visite seus sites para se inscrever e obter chaves de API.

**2. Defina sua chave de API como vari√°vel de ambiente:**

*   **Linux/macOS:**
    Abra um terminal e use o comando `export`. √â melhor adicion√°-lo ao arquivo de configura√ß√£o do seu shell (ex: `~/.bashrc`, `~/.zshrc`) para que seja persistente.
    ```sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # Substitua PROVIDER_API_KEY pelo nome de vari√°vel espec√≠fico, ex: OPENAI_API_KEY, GOOGLE_API_KEY
    ```
    Exemplo TogetherAI:
    ```sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    ```
*   **Windows:**
    *   **Prompt de comando (tempor√°rio para a sess√£o atual):**
        ```cmd
        set PROVIDER_API_KEY=your_api_key_here
        ```
    *   **PowerShell (tempor√°rio para a sess√£o atual):**
        ```powershell
        $env:PROVIDER_API_KEY="your_api_key_here"
        ```
    *   **Permanente:** Pesquise "vari√°veis de ambiente" na barra de pesquisa do Windows, clique em "Editar vari√°veis de ambiente do sistema" e depois no bot√£o "Vari√°veis de ambiente...". Adicione uma nova vari√°vel de usu√°rio com o nome apropriado (ex: `OPENAI_API_KEY`) e sua chave como valor.

    *(Para mais detalhes, consulte o FAQ: [Como configurar uma chave de API?](#como-configurar-uma-chave-de-api)).*


**3. Atualize `config.ini`:**
```ini
[MAIN]
is_local = False
provider_name = openai # ou google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1, etc.
provider_server_address = # Quando is_local = False, geralmente ignorado ou pode ser deixado vazio para a maioria das APIs
# ... outras configura√ß√µes ...
```
*Aviso:* Certifique-se de que n√£o h√° espa√ßos no final dos valores no config.

**Lista de provedores de API**

| Provedor     | `provider_name` | Local ? | Descri√ß√£o                                       | Link da chave de API (exemplo)                     |
|--------------|-----------------|--------|---------------------------------------------------|---------------------------------------------|
| OpenAI       | `openai`        | N√£o     | Use os modelos ChatGPT via API OpenAI.              | [platform.openai.com/signup](https://platform.openai.com/signup) |
| Google Gemini| `google`        | N√£o     | Use os modelos Google Gemini via Google AI Studio.    | [aistudio.google.com/keys](https://aistudio.google.com/keys) |
| Deepseek     | `deepseek`      | N√£o     | Use os modelos Deepseek via sua API.                | [platform.deepseek.com](https://platform.deepseek.com) |
| Hugging Face | `huggingface`   | N√£o     | Use modelos do Hugging Face Inference API.       | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |
| TogetherAI   | `togetherAI`    | N√£o     | Use v√°rios modelos open source via API TogetherAI.| [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) |

*Nota:*
*   N√£o recomendamos usar `gpt-4o` ou outros modelos OpenAI para navega√ß√£o web complexa e planejamento de tarefas, pois a otimiza√ß√£o atual de prompts visa modelos como Deepseek.
*   Tarefas de codifica√ß√£o/bash podem falhar com Gemini, pois tende a ignorar nosso formato de prompt otimizado para Deepseek r1.
*   Quando `is_local = False`, `provider_server_address` no `config.ini` geralmente n√£o √© usado, pois os endpoints de API s√£o geralmente gerenciados pelas bibliotecas do provedor correspondente.

Pr√≥xima etapa: [Iniciar os servi√ßos e executar o AgenticSeek](#iniciar-os-servi√ßos-e-executar)

*Se voc√™ encontrar problemas, consulte a se√ß√£o **Problemas conhecidos***

*Para explica√ß√µes detalhadas do arquivo de configura√ß√£o, consulte a **se√ß√£o Configura√ß√£o**.*

---

## Iniciar os servi√ßos e executar

Por padr√£o, o AgenticSeek √© executado inteiramente no Docker.

**Op√ß√£o 1:** Executar no Docker com interface web:

Inicie os servi√ßos necess√°rios. Isso iniciar√° todos os servi√ßos do docker-compose.yml, incluindo:
    - searxng
    - redis (necess√°rio para searxng)
    - frontend
    - backend (se voc√™ usar `full` para a interface web)

```sh
./start_services.sh full # MacOS
start start_services.cmd full # Windows
```

**Aviso:** Esta etapa baixar√° e carregar√° todas as imagens do Docker, o que pode levar at√© 30 minutos. Depois de iniciar os servi√ßos, aguarde at√© que o servi√ßo backend esteja totalmente operacional (voc√™ deve ver **backend: "GET /health HTTP/1.1" 200 OK** nos logs) antes de enviar mensagens. Na primeira inicializa√ß√£o, o servi√ßo backend pode levar 5 minutos para iniciar.

V√° para `http://localhost:3000/` e voc√™ deve ver a interface web.

*Solu√ß√£o de problemas de inicializa√ß√£o de servi√ßos:* Se esses scripts falharem, certifique-se de que o Docker Engine est√° funcionando e que o Docker Compose (V2, `docker compose`) est√° instalado corretamente. Verifique as mensagens de erro na sa√≠da do terminal. Consulte [FAQ: Ajuda! Estou recebendo erros ao executar o AgenticSeek ou seus scripts.](#faq-solu√ß√£o-de-problemas)

**Op√ß√£o 2:** Modo CLI:

Para executar com a interface CLI, voc√™ precisa instalar os pacotes no host:

```sh
./install.sh
./install.bat # windows
```

Em seguida, voc√™ precisa alterar SEARXNG_BASE_URL no `config.ini` para:

```sh
SEARXNG_BASE_URL="http://localhost:8080"
```

Inicie os servi√ßos necess√°rios. Isso iniciar√° alguns servi√ßos do docker-compose.yml, incluindo:
    - searxng
    - redis (necess√°rio para searxng)
    - frontend

```sh
./start_services.sh # MacOS
start start_services.cmd # Windows
```

Execute: uv run: `uv run python -m ensurepip` para garantir que o uv tenha o pip ativado.

Use CLI: `uv run cli.py`

---

## Uso

Certifique-se de que os servi√ßos est√£o funcionando com `./start_services.sh full` e v√° para `localhost:3000` para a interface web.

Voc√™ tamb√©m pode usar fala para texto definindo `listen = True`. Apenas para o modo CLI.

Para sair, basta dizer/digitar `goodbye`.

Alguns exemplos de uso:

> *Fa√ßa um jogo da cobra em python!*

> *Pesquise na web os melhores caf√©s em Rennes, Fran√ßa, e salve uma lista de tr√™s com seus endere√ßos em rennes_cafes.txt.*

> *Escreva um programa Go para calcular o fatorial de um n√∫mero, salve-o como factorial.go em seu workspace*

> *Pesquise na pasta summer_pictures todos os arquivos JPG, renomeie-os com a data de hoje e salve a lista de arquivos renomeados em photos_list.txt*

> *Pesquise online os filmes de fic√ß√£o cient√≠fica populares de 2024 e escolha tr√™s para assistir esta noite. Salve a lista em movie_night.txt.*

> *Pesquise na web os √∫ltimos artigos de not√≠cias sobre IA de 2025, selecione tr√™s e escreva um script Python para extrair os t√≠tulos e resumos. Salve o script como news_scraper.py e os resumos em ai_news.txt em /home/projects*

> *Sexta-feira, pesquise na web uma API gratuita de pre√ßos de a√ß√µes, inscreva-se com supersuper7434567@gmail.com e escreva um script Python para obter os pre√ßos di√°rios da Tesla usando a API, salvando os resultados em stock_prices.csv*

*Observe que o preenchimento de formul√°rios ainda √© experimental e pode falhar.*

Depois de inserir sua consulta, o AgenticSeek atribuir√° o melhor agente para a tarefa.

Como este √© um prot√≥tipo inicial, o sistema de roteamento de agentes pode nem sempre atribuir o agente correto √† sua consulta.

Portanto, seja muito expl√≠cito sobre o que voc√™ quer e como a IA pode proceder, por exemplo, se voc√™ quiser que ela fa√ßa uma pesquisa na web, n√£o diga:

`Voc√™ conhece bons pa√≠ses para viajar sozinho?`

Em vez disso, diga:

`Execute uma pesquisa na web e descubra quais s√£o os melhores pa√≠ses para viajar sozinho`

---

## **Configura√ß√£o para executar LLM em seu pr√≥prio servidor**

Se voc√™ tem um computador poderoso ou um servidor que pode acessar, mas quer us√°-lo do seu laptop, voc√™ pode optar por executar o LLM em um servidor remoto usando nosso servidor llm personalizado.

No seu "servidor" que executar√° o modelo de IA, obtenha o endere√ßo IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # IP local
curl https://ipinfo.io/ip # IP p√∫blico
```

Nota: Para Windows ou macOS, use ipconfig ou ifconfig para encontrar o endere√ßo IP.

Clone o reposit√≥rio e entre na pasta `server/`.

```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Instale os requisitos espec√≠ficos do servidor:

```sh
pip3 install -r requirements.txt
```

Execute o script do servidor.

```sh
python3 app.py --provider ollama --port 3333
```

Voc√™ pode escolher usar `ollama` e `llamacpp` como servi√ßo LLM.

Agora no seu computador pessoal:

Altere o arquivo `config.ini` para definir `provider_name` como `server` e `provider_model` como `deepseek-r1:xxb`.
Defina `provider_server_address` como o endere√ßo IP da m√°quina que executar√° o modelo.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333
```

Pr√≥xima etapa: [Iniciar os servi√ßos e executar o AgenticSeek](#iniciar-os-servi√ßos-e-executar)  

---

## Fala para Texto

Aviso: Fala para texto funciona apenas no modo CLI no momento.

Observe que a fala para texto funciona apenas em ingl√™s no momento.

A funcionalidade de fala para texto est√° desativada por padr√£o. Para ativ√°-la, defina listen como True no arquivo config.ini:

```
listen = True
```

Quando ativada, a funcionalidade de fala para texto ouve uma palavra-chave de gatilho, que √© o nome do agente, antes de processar sua entrada. Voc√™ pode personalizar o nome do agente atualizando o valor `agent_name` em *config.ini*:

```
agent_name = Friday
```

Para melhor reconhecimento, recomendamos usar um nome comum em ingl√™s como "John" ou "Emma" como nome do agente.

Assim que voc√™ vir a transcri√ß√£o come√ßar a aparecer, diga o nome do agente em voz alta para acord√°-lo (ex: "Friday").

Diga sua consulta claramente.

Termine sua solicita√ß√£o com uma frase de confirma√ß√£o para indicar ao sistema para continuar. Exemplos de frases de confirma√ß√£o incluem:
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Configura√ß√£o

Exemplo de configura√ß√£o:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Exemplo Ollama; LM-Studio usa http://127.0.0.1:1234
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Lista de idiomas para TTS e roteamento potencial.
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explica√ß√£o das configura√ß√µes de `config.ini`**:

*   **Se√ß√£o `[MAIN]`:**
    *   `is_local`: `True` se voc√™ estiver usando provedores de LLM locais (Ollama, LM-Studio, servidor local compat√≠vel com OpenAI) ou a op√ß√£o de servidor auto-hospedado. `False` se voc√™ estiver usando APIs baseadas em nuvem (OpenAI, Google, etc.).
    *   `provider_name`: Especifica o provedor de LLM.
        *   Op√ß√µes locais: `ollama`, `lm-studio`, `openai` (para servidor local compat√≠vel com OpenAI), `server` (para configura√ß√£o de servidor auto-hospedado).
        *   Op√ß√µes de API: `openai`, `google`, `deepseek`, `huggingface`, `togetherAI`.
    *   `provider_model`: Nome ou ID espec√≠fico do modelo do provedor selecionado (ex: `deepseekcoder:6.7b` para Ollama, `gpt-3.5-turbo` para API OpenAI, `mistralai/Mixtral-8x7B-Instruct-v0.1` para TogetherAI).
    *   `provider_server_address`: O endere√ßo do seu provedor de LLM.
        *   Para provedores locais: ex: `http://127.0.0.1:11434` para Ollama, `http://127.0.0.1:1234` para LM-Studio.
        *   Para o tipo de provedor `server`: O endere√ßo do seu servidor LLM auto-hospedado (ex: `http://your_server_ip:3333`).
        *   Para APIs em nuvem (`is_local = False`): Isso geralmente √© ignorado ou pode ser deixado em branco, pois os endpoints da API s√£o geralmente gerenciados pelas bibliotecas do provedor correspondente.
    *   `agent_name`: O nome do assistente de IA (ex: Friday). Se ativado, usado como palavra de gatilho para fala para texto.
    *   `recover_last_session`: `True` para tentar recuperar o estado da sess√£o anterior, `False` para come√ßar do zero.
    *   `save_session`: `True` para salvar o estado da sess√£o atual para poss√≠vel recupera√ß√£o, `False` caso contr√°rio.
    *   `speak`: `True` para ativar a sa√≠da de voz de texto para fala, `False` para desativar.
    *   `listen`: `True` para ativar a entrada de voz de fala para texto (apenas modo CLI), `False` para desativar.
    *   `work_dir`: **Cr√≠tico:** O diret√≥rio onde o AgenticSeek ler√°/escrever√° arquivos. **Certifique-se de que este caminho √© v√°lido e acess√≠vel no seu sistema.**
    *   `jarvis_personality`: `True` para usar prompts de sistema mais "Jarvis-like" (experimental), `False` para usar prompts padr√£o.
    *   `languages`: Lista de idiomas separados por v√≠rgulas (ex: `en, zh, fr`). Usado para sele√ß√£o de voz TTS (primeira por padr√£o) e pode ajudar o roteador LLM. Para evitar inefici√™ncias do roteador, evite usar muitos idiomas ou idiomas muito semelhantes.
*   **Se√ß√£o `[BROWSER]`:**
    *   `headless_browser`: `True` para executar o navegador automatizado sem janela vis√≠vel (recomendado para interface web ou uso n√£o interativo). `False` para exibir a janela do navegador (√∫til para modo CLI ou depura√ß√£o).
    *   `stealth_mode`: `True` para ativar medidas que tornam mais dif√≠cil a detec√ß√£o da automa√ß√£o do navegador. Pode exigir instala√ß√£o manual de extens√µes de navegador como anticaptcha.

Esta se√ß√£o resume os tipos de provedores de LLM suportados. Configure-os em `config.ini`.

**Provedores locais (executando em seu pr√≥prio hardware):**

| Nome do provedor em config.ini | `is_local` | Descri√ß√£o                                                                 | Se√ß√£o de configura√ß√£o                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|------------------------------------------------------------------|
| `ollama`                      | `True`     | Fornece LLM localmente facilmente usando Ollama.                                             | [Configura√ß√£o para executar LLM localmente na sua m√°quina](#configura√ß√£o-para-executar-llm-localmente-na-sua-m√°quina) |
| `lm-studio`                   | `True`     | Fornece LLM localmente com LM-Studio.                                          | [Configura√ß√£o para executar LLM localmente na sua m√°quina](#configura√ß√£o-para-executar-llm-localmente-na-sua-m√°quina) |
| `openai` (para servidor local)   | `True`     | Conecte-se a um servidor local expondo uma API compat√≠vel com OpenAI (ex: llama.cpp). | [Configura√ß√£o para executar LLM localmente na sua m√°quina](#configura√ß√£o-para-executar-llm-localmente-na-sua-m√°quina) |
| `server`                      | `False`    | Conecte-se ao servidor LLM auto-hospedado do AgenticSeek em execu√ß√£o em outra m√°quina. | [Configura√ß√£o para executar LLM em seu pr√≥prio servidor](#configura√ß√£o-para-executar-llm-em-seu-pr√≥prio-servidor) |

**Provedores de API (baseados em nuvem):**

| Nome do provedor em config.ini | `is_local` | Descri√ß√£o                                      | Se√ß√£o de configura√ß√£o                                       |
|-------------------------------|------------|--------------------------------------------------|-----------------------------------------------------|
| `openai`                      | `False`    | Use a API oficial da OpenAI (ex: GPT-3.5, GPT-4). | [Configura√ß√£o para executar com uma API](#configura√ß√£o-para-executar-com-uma-api) |
| `google`                      | `False`    | Use os modelos Google Gemini via API.              | [Configura√ß√£o para executar com uma API](#configura√ß√£o-para-executar-com-uma-api) |
| `deepseek`                    | `False`    | Use a API oficial da Deepseek.                     | [Configura√ß√£o para executar com uma API](#configura√ß√£o-para-executar-com-uma-api) |
| `huggingface`                 | `False`    | Use Hugging Face Inference API.                  | [Configura√ß√£o para executar com uma API](#configura√ß√£o-para-executar-com-uma-api) |
| `togetherAI`                  | `False`    | Use v√°rios modelos abertos via API TogetherAI.    | [Configura√ß√£o para executar com uma API](#configura√ß√£o-para-executar-com-uma-api) |

---
## Solu√ß√£o de problemas

Se voc√™ encontrar problemas, esta se√ß√£o fornece orienta√ß√µes.

# Problemas conhecidos

## Problemas do ChromeDriver

**Exemplo de erro:** `SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX`

### Causa raiz
A incompatibilidade de vers√£o do ChromeDriver ocorre quando:
1. A vers√£o do ChromeDriver que voc√™ instalou n√£o corresponde √† vers√£o do navegador Chrome
2. Em ambientes Docker, `undetected_chromedriver` pode baixar sua pr√≥pria vers√£o do ChromeDriver, contornando os bin√°rios montados

### Etapas de solu√ß√£o

#### 1. Verifique sua vers√£o do Chrome
Abra o Google Chrome ‚Üí `Configura√ß√µes > Sobre o Chrome` para encontrar sua vers√£o (ex: "Vers√£o 134.0.6998.88")

#### 2. Baixe o ChromeDriver correspondente

**Para Chrome 115 e superior:** Use [Chrome for Testing API](https://googlechromelabs.github.io/chrome-for-testing/)
- Visite o painel de disponibilidade do Chrome for Testing
- Encontre sua vers√£o do Chrome ou a correspond√™ncia dispon√≠vel mais pr√≥xima
- Baixe o ChromeDriver para seu sistema operacional (use Linux64 para ambientes Docker)

**Para vers√µes mais antigas do Chrome:** Use [Downloads legados do ChromeDriver](https://chromedriver.chromium.org/downloads)

![Baixar ChromeDriver do Chrome for Testing](./media/chromedriver_readme.png)

#### 3. Instale o ChromeDriver (escolha um m√©todo)

**M√©todo A: Diret√≥rio raiz do projeto (recomendado para Docker)**
```bash
# Coloque o bin√°rio chromedriver baixado no diret√≥rio raiz do projeto
cp path/to/downloaded/chromedriver ./chromedriver
chmod +x ./chromedriver  # Torne-o execut√°vel no Linux/macOS
```

**M√©todo B: PATH do sistema**
```bash
# Linux/macOS
sudo mv chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver

# Windows: Coloque chromedriver.exe em uma pasta do PATH
```

#### 4. Verifique a instala√ß√£o
```bash
# Teste a vers√£o do ChromeDriver
./chromedriver --version
# Ou se estiver no PATH:
chromedriver --version
```

### Instru√ß√µes espec√≠ficas do Docker

‚ö†Ô∏è **Importante para usu√°rios do Docker:**
- O m√©todo de montagem de volumes do Docker pode n√£o funcionar com o modo furtivo (`undetected_chromedriver`)
- **Solu√ß√£o:** Coloque o ChromeDriver no diret√≥rio raiz do projeto como `./chromedriver`
- O aplicativo o detectar√° automaticamente e usar√° este bin√°rio
- Voc√™ deve ver nos logs: `"Using ChromeDriver from project root: ./chromedriver"`

### Dicas de solu√ß√£o de problemas

1. **Ainda h√° incompatibilidade de vers√£o?**
   - Verifique se o ChromeDriver √© execut√°vel: `ls -la ./chromedriver`
   - Verifique a vers√£o do ChromeDriver: `./chromedriver --version`
   - Certifique-se de que corresponde √† vers√£o do seu navegador Chrome

2. **Problemas com o cont√™iner Docker?**
   - Verifique os logs do backend: `docker logs backend`
   - Procure a mensagem: `"Using ChromeDriver from project root"`
   - Se n√£o for encontrado, verifique se o arquivo existe e √© execut√°vel

3. **Vers√µes do Chrome for Testing**
   - Use uma correspond√™ncia exata quando poss√≠vel
   - Para a vers√£o 134.0.6998.88, use o ChromeDriver 134.0.6998.165 (a vers√£o dispon√≠vel mais pr√≥xima)
   - O n√∫mero da vers√£o principal deve corresponder (134 = 134)

### Matriz de compatibilidade de vers√µes

| Vers√£o do Chrome | Vers√£o do ChromeDriver | Status |
|----------------|---------------------|---------|
| 134.0.6998.x   | 134.0.6998.165     | ‚úÖ Dispon√≠vel |
| 133.0.6943.x   | 133.0.6943.141     | ‚úÖ Dispon√≠vel |
| 132.0.6834.x   | 132.0.6834.159     | ‚úÖ Dispon√≠vel |

*Para a compatibilidade mais recente, consulte o [Painel do Chrome for Testing](https://googlechromelabs.github.io/chrome-for-testing/)*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Isso acontece se seu navegador e a vers√£o do chromedriver n√£o corresponderem.

Voc√™ precisa navegar para baixar a vers√£o mais recente:

https://developer.chrome.com/docs/chromedriver/downloads

Se voc√™ estiver usando o Chrome vers√£o 115 ou superior, v√° para:

https://googlechromelabs.github.io/chrome-for-testing/

e baixe a vers√£o do chromedriver correspondente ao seu sistema operacional.

![alt text](./media/chromedriver_readme.png)

Se esta se√ß√£o estiver incompleta, abra um issue.

##  Problemas de adaptadores de conex√£o

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (nota: a porta pode variar)
```

*   **Causa:** Falta o prefixo `http://` em `provider_server_address` para `lm-studio` (ou outro servidor local compat√≠vel com OpenAI semelhante) no `config.ini`, ou ele aponta para a porta errada.
*   **Solu√ß√£o:**
    *   Certifique-se de que o endere√ßo inclui `http://`. O LM-Studio geralmente usa `http://127.0.0.1:1234` por padr√£o.
    *   `config.ini` correto: `provider_server_address = http://127.0.0.1:1234` (ou sua porta real do servidor LM-Studio).

## URL base do SearxNG n√£o fornecida

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
```

Isso pode acontecer se voc√™ executar o modo CLI com uma URL base do searxng incorreta.

SEARXNG_BASE_URL deve diferir dependendo se voc√™ est√° executando no Docker ou no host:

**Execu√ß√£o no host:** `SEARXNG_BASE_URL="http://localhost:8080"`

**Execu√ß√£o completamente no Docker (interface web):** `SEARXNG_BASE_URL="http://searxng:8080"`

## FAQ

**P: De qual hardware eu preciso?**  

| Tamanho do modelo  | GPU  | Coment√°rios                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB VRAM | ‚ö†Ô∏è N√£o recomendado. Desempenho ruim, alucina√ß√µes frequentes, agentes de planejamento podem falhar. |
| 14B        | 12 GB VRAM (ex: RTX 3060) | ‚úÖ Utiliz√°vel para tarefas simples. Pode ter dificuldades com navega√ß√£o web e planejamento de tarefas. |
| 32B        | 24+ GB VRAM (ex: RTX 4090) | üöÄ Consegue a maioria das tarefas, ainda pode ter dificuldades com planejamento de tarefas |
| 70B+        | 48+ GB VRAM | üí™ Excelente. Recomendado para casos de uso avan√ßados. |

**P: O que fazer se eu encontrar erros?**  

Certifique-se de que o local est√° funcionando (`ollama serve`), que seu `config.ini` corresponde ao seu provedor e que as depend√™ncias est√£o instaladas. Se nada funcionar, sinta-se √† vontade para abrir um issue.

**P: Ele pode realmente funcionar 100% localmente?**  

Sim, com os provedores Ollama, lm-studio ou server, todos os modelos de fala para texto, LLM e texto para fala funcionam localmente. As op√ß√µes n√£o locais (OpenAI ou outras APIs) s√£o opcionais.

**P: Por que eu deveria usar o AgenticSeek quando tenho o Manus?**

Ao contr√°rio do Manus, o AgenticSeek prioriza a independ√™ncia de sistemas externos, dando a voc√™ mais controle, privacidade e evitando custos de API.

**P: Quem est√° por tr√°s deste projeto?**

Este projeto foi criado por mim, com dois amigos como mantenedores e contribuidores da comunidade de c√≥digo aberto no GitHub. Somos apenas indiv√≠duos apaixonados, n√£o uma startup, nem afiliados a qualquer organiza√ß√£o.

Qualquer conta AgenticSeek no X diferente da minha conta pessoal (https://x.com/Martin993886460) √© um impostor.

## Contribuir

Estamos procurando desenvolvedores para melhorar o AgenticSeek! Verifique os problemas abertos ou discuss√µes.

[Guia de contribui√ß√£o](./docs/CONTRIBUTING.md)

## Patrocinadores:

Voc√™ quer melhorar as capacidades do AgenticSeek com recursos como pesquisa de voos, planejamento de viagens ou obten√ß√£o das melhores ofertas de compra? Considere usar o SerpApi para criar ferramentas personalizadas que desbloqueiem mais recursos do tipo Jarvis. Com o SerpApi, voc√™ pode acelerar seu agente para tarefas profissionais enquanto mant√©m o controle total.

<a href="https://serpapi.com/"><img src="./media/banners/sponsor_banner_serpapi.png" height="350" alt="SerpApi Banner" ></a>

Confira [Contributing.md](./docs/CONTRIBUTING.md) para aprender como integrar ferramentas personalizadas!

### **Patrocinadores**:

- [tatra-labs](https://github.com/tatra-labs)

## Mantenedores:

 > [Fosowl](https://github.com/Fosowl) | Hor√°rio de Paris 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Hor√°rio de Taipei 

## Agradecimentos especiais:

 > [tcsenpai](https://github.com/tcsenpai) e [plitc](https://github.com/plitc) por ajudar na dockeriza√ß√£o do backend

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)
