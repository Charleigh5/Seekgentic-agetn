# AgenticSeek: Una Alternativa Privada y Local a Manus

<p align="center">
<img align="center" src="./media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

  English | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | [Fran√ßais](./README_FR.md) | [Êó•Êú¨Ë™û](./README_JP.md) | [Portugu√™s (Brasil)](./README_PTBR.md) | [Espa√±ol](./README_ES.md)

*Un asistente de IA con capacidad de voz que es una **alternativa 100% local a Manus AI**, navega aut√≥nomamente por la web, escribe c√≥digo y planifica tareas manteniendo todos los datos en tu dispositivo. Dise√±ado para modelos de razonamiento local, funciona completamente en tu hardware, garantizando privacidad total y cero dependencia de la nube.*

[![Visitar AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### ¬øPor qu√© AgenticSeek?

* üîí Totalmente Local & Privado - Todo funciona en tu m√°quina, sin nube, sin compartir datos. Tus archivos, conversaciones y b√∫squedas permanecen privados.

* üåê Navegaci√≥n Web Inteligente - AgenticSeek puede navegar por Internet de forma aut√≥noma: buscar, leer, extraer informaci√≥n, completar formularios web, todo sin manos.

* üíª Asistente de Programaci√≥n Aut√≥nomo - ¬øNecesitas c√≥digo? Puede escribir, depurar y ejecutar programas en Python, C, Go, Java y m√°s, sin supervisi√≥n.

* üß† Selecci√≥n Inteligente de Agentes - T√∫ pides, √©l elige autom√°ticamente el mejor agente para la tarea. Como tener un equipo de expertos siempre disponible.

* üìã Planifica y Ejecuta Tareas Complejas - Desde planificaci√≥n de viajes hasta proyectos complejos, puede dividir grandes tareas en pasos y completarlos utilizando m√∫ltiples agentes de IA.

* üéôÔ∏è Compatibilidad con Voz - Voz limpia, r√°pida y futurista con reconocimiento de voz, permiti√©ndote conversar como si fuera tu IA personal de una pel√≠cula de ciencia ficci√≥n. (En desarrollo)

### **Demo**

> *¬øPuedes buscar el proyecto agenticSeek, aprender qu√© habilidades se necesitan, luego abrir CV_candidates.zip y decirme cu√°les coinciden mejor con el proyecto?*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Descargo de responsabilidad: Esta demostraci√≥n y todos los archivos que aparecen (ej: CV_candidates.zip) son completamente ficticios. No somos una corporaci√≥n, buscamos colaboradores de c√≥digo abierto, no candidatos.

> üõ†‚ö†Ô∏èÔ∏è **Trabajo Activo en Progreso**

> üôè Este proyecto comenz√≥ como un proyecto paralelo y no tiene hoja de ruta ni financiaci√≥n. Creci√≥ mucho m√°s all√° de lo esperado al aparecer en GitHub Trending. Las contribuciones, comentarios y paciencia son profundamente apreciados.

## Prerrequisitos

Antes de comenzar, aseg√∫rate de tener instalado:

*   **Git:** Para clonar el repositorio. [Descargar Git](https://git-scm.com/downloads)
*   **Python 3.10.x:** Se recomienda encarecidamente Python 3.10.x. Otras versiones pueden causar errores de dependencia. [Descargar Python 3.10](https://www.python.org/downloads/release/python-3100/) (selecciona la versi√≥n 3.10.x).
*   **Docker Engine & Docker Compose:** Para ejecutar servicios empaquetados como SearxNG.
    *   Instalar Docker Desktop (incluye Docker Compose V2): [Windows](https://docs.docker.com/desktop/install/windows-install/) | [Mac](https://docs.docker.com/desktop/install/mac-install/) | [Linux](https://docs.docker.com/desktop/install/linux-install/)
    *   O instalar Docker Engine y Docker Compose por separado en Linux: [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/) (aseg√∫rate de instalar Compose V2, por ejemplo `sudo apt-get install docker-compose-plugin`).

### 1. **Clonar el repositorio y configurar**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Modificar el contenido del archivo .env

```sh
SEARXNG_BASE_URL="http://searxng:8080" # Si ejecutas en modo CLI en el host, usa http://127.0.0.1:8080
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
```

Actualiza el archivo `.env` seg√∫n sea necesario:

- **SEARXNG_BASE_URL**: Mantener sin cambios a menos que ejecutes en modo CLI en el host.
- **REDIS_BASE_URL**: Mantener sin cambios 
- **WORK_DIR**: Ruta al directorio de trabajo local. AgenticSeek podr√° leer e interactuar con estos archivos.
- **OLLAMA_PORT**: N√∫mero de puerto para el servicio Ollama.
- **LM_STUDIO_PORT**: N√∫mero de puerto para el servicio LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Puerto para cualquier servicio LLM adicional personalizado.

**Las claves API son completamente opcionales para quienes optan por ejecutar LLM localmente, que es el objetivo principal de este proyecto. D√©jalas en blanco si tienes hardware suficiente.**

### 3. **Iniciar Docker**

Aseg√∫rate de que Docker est√© instalado y ejecut√°ndose en tu sistema. Puedes iniciar Docker con los siguientes comandos:

- **Linux/macOS:**  
    Abre una terminal y ejecuta:
    ```sh
    sudo systemctl start docker
    ```
    O inicia Docker Desktop desde el men√∫ de aplicaciones, si est√° instalado.

- **Windows:**  
    Inicia Docker Desktop desde el men√∫ Inicio.

Puedes verificar si Docker se est√° ejecutando ejecutando:
```sh
docker info
```
Si ves informaci√≥n sobre tu instalaci√≥n de Docker, est√° funcionando correctamente.

Consulta la [Lista de proveedores locales](#lista-de-proveedores-locales) a continuaci√≥n para obtener un resumen.

Siguiente paso: [Ejecutar AgenticSeek localmente](#iniciar-servicios-y-ejecutar)

*Si tienes problemas, consulta la secci√≥n [Soluci√≥n de problemas](#soluci√≥n-de-problemas).*
*Si tu hardware no puede ejecutar LLM localmente, consulta [Configuraci√≥n para ejecutar con una API](#configuraci√≥n-para-ejecutar-con-una-api).*
*Para explicaciones detalladas de `config.ini`, consulta la [secci√≥n Configuraci√≥n](#configuraci√≥n).*

---

## Configuraci√≥n para ejecutar LLM localmente en tu m√°quina

**Requisitos de hardware:**

Para ejecutar LLM localmente, necesitar√°s hardware suficiente. Como m√≠nimo, se requiere una GPU capaz de ejecutar Magistral, Qwen o Deepseek 14B. Consulta el FAQ para recomendaciones detalladas de modelo/rendimiento.

**Configura tu proveedor local**  

Inicia tu proveedor local, por ejemplo con ollama:

```sh
ollama serve
```

Consulta la lista de proveedores locales admitidos a continuaci√≥n.

**Actualizar config.ini**

Cambia el archivo config.ini para establecer provider_name en un proveedor admitido y provider_model en un LLM admitido por tu proveedor. Recomendamos modelos de razonamiento como *Magistral* o *Deepseek*.

Consulta el **FAQ** al final del README para el hardware necesario.

```sh
[MAIN]
is_local = True # Ya sea que ejecutes localmente o con un proveedor remoto.
provider_name = ollama # o lm-studio, openai, etc.
provider_model = deepseek-r1:14b # elige un modelo compatible con tu hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # el nombre de tu IA
recover_last_session = True # recuperar sesi√≥n anterior
save_session = True # recordar sesi√≥n actual
speak = False # texto a voz
listen = False # voz a texto, solo para CLI, experimental
jarvis_personality = False # usar personalidad m√°s "Jarvis" (experimental)
languages = en zh # Lista de idiomas, TTS usar√° el primero de la lista por defecto
[BROWSER]
headless_browser = True # mantener sin cambios a menos que uses CLI en el host.
stealth_mode = True # Usa selenium indetectable para reducir la detecci√≥n del navegador
```

**Advertencia**:

- El formato del archivo `config.ini` no admite comentarios.
No copies y pegues la configuraci√≥n de ejemplo directamente, ya que los comentarios causar√°n errores. En su lugar, modifica manualmente el archivo `config.ini` con tu configuraci√≥n deseada, sin comentarios.

- *NO* establezcas provider_name como `openai` si est√°s usando LM-studio para ejecutar LLM. √ösalo como `lm-studio`.

- Algunos proveedores (ej: lm-studio) requieren `http://` antes de la IP. Ejemplo: `http://127.0.0.1:1234`

**Lista de proveedores locales**

| Proveedor  | ¬øLocal? | Descripci√≥n                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | S√≠    | Ejecuta LLM localmente f√°cilmente usando ollama |
| lm-studio  | S√≠    | Ejecuta LLM localmente con LM studio (establecer `provider_name` = `lm-studio`)|
| openai    | S√≠     |  Usa API compatible con openai (ej: servidor llama.cpp)  |

Siguiente paso: [Iniciar servicios y ejecutar AgenticSeek](#iniciar-servicios-y-ejecutar)  

*Si tienes problemas, consulta la secci√≥n [Soluci√≥n de problemas](#soluci√≥n-de-problemas).*
*Si tu hardware no puede ejecutar LLM localmente, consulta [Configuraci√≥n para ejecutar con una API](#configuraci√≥n-para-ejecutar-con-una-api).*
*Para explicaciones detalladas de `config.ini`, consulta la [secci√≥n Configuraci√≥n](#configuraci√≥n).*

## Configuraci√≥n para ejecutar con una API

Esta configuraci√≥n utiliza proveedores de LLM externos basados en la nube. Necesitar√°s obtener claves API del servicio elegido.

**1. Elige un proveedor de API y obt√©n una clave API:**

Consulta la [Lista de proveedores de API](#lista-de-proveedores-de-api) a continuaci√≥n. Visita sus sitios web para registrarte y obtener claves API.

**2. Establece tu clave API como variable de entorno:**

*   **Linux/macOS:**
    Abre una terminal y usa el comando `export`. Es mejor agregarlo al archivo de configuraci√≥n de tu shell (ej: `~/.bashrc`, `~/.zshrc`) para que sea persistente.
    ```sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # Reemplaza PROVIDER_API_KEY con el nombre de variable espec√≠fico, ej: OPENAI_API_KEY, GOOGLE_API_KEY
    ```
    Ejemplo de TogetherAI:
    ```sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    ```
*   **Windows:**
    *   **S√≠mbolo del sistema (temporal para la sesi√≥n actual):**
        ```cmd
        set PROVIDER_API_KEY=your_api_key_here
        ```
    *   **PowerShell (temporal para la sesi√≥n actual):**
        ```powershell
        $env:PROVIDER_API_KEY="your_api_key_here"
        ```
    *   **Permanente:** Busca "variables de entorno" en la barra de b√∫squeda de Windows, haz clic en "Editar las variables de entorno del sistema", luego en el bot√≥n "Variables de entorno...". Agrega una nueva variable de usuario con el nombre apropiado (ej: `OPENAI_API_KEY`) y tu clave como valor.

    *(Para m√°s detalles, consulta el FAQ: [¬øC√≥mo configuro una clave API?](#c√≥mo-configuro-una-clave-api)).*


**3. Actualiza `config.ini`:**
```ini
[MAIN]
is_local = False
provider_name = openai # o google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # o gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1, etc.
provider_server_address = # Cuando is_local = False, generalmente se ignora o puede dejarse en blanco para la mayor√≠a de las API
# ... otras configuraciones ...
```
*Advertencia:* Aseg√∫rate de que no haya espacios al final de los valores en config.

**Lista de proveedores de API**

| Proveedor     | `provider_name` | ¬øLocal? | Descripci√≥n                                       | Enlace de clave API (ejemplo)                     |
|--------------|-----------------|--------|---------------------------------------------------|---------------------------------------------|
| OpenAI       | `openai`        | No     | Usa modelos ChatGPT a trav√©s de la API de OpenAI.              | [platform.openai.com/signup](https://platform.openai.com/signup) |
| Google Gemini| `google`        | No     | Usa modelos Google Gemini a trav√©s de Google AI Studio.    | [aistudio.google.com/keys](https://aistudio.google.com/keys) |
| Deepseek     | `deepseek`      | No     | Usa modelos Deepseek a trav√©s de su API.                | [platform.deepseek.com](https://platform.deepseek.com) |
| Hugging Face | `huggingface`   | No     | Usa modelos del Hugging Face Inference API.       | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |
| TogetherAI   | `togetherAI`    | No     | Usa varios modelos de c√≥digo abierto a trav√©s de la API de TogetherAI.| [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) |

*Nota:*
*   No recomendamos usar `gpt-4o` u otros modelos OpenAI para navegaci√≥n web compleja y planificaci√≥n de tareas, ya que la optimizaci√≥n actual de prompts est√° dirigida a modelos como Deepseek.
*   Las tareas de codificaci√≥n/bash pueden fallar con Gemini, ya que tiende a ignorar nuestro formato de prompt optimizado para Deepseek r1.
*   Cuando `is_local = False`, `provider_server_address` en `config.ini` generalmente no se usa, ya que los endpoints de API suelen estar codificados en las bibliotecas del proveedor correspondiente.

Siguiente paso: [Iniciar servicios y ejecutar AgenticSeek](#iniciar-servicios-y-ejecutar)

*Si tienes problemas, consulta la secci√≥n **Problemas conocidos***

*Para explicaciones detalladas del archivo de configuraci√≥n, consulta la **secci√≥n Configuraci√≥n**.*

---

## Iniciar servicios y ejecutar

Por defecto, AgenticSeek se ejecuta completamente en Docker.

**Opci√≥n 1:** Ejecutar en Docker con interfaz web:

Inicia los servicios necesarios. Esto iniciar√° todos los servicios del docker-compose.yml, incluyendo:
    - searxng
    - redis (requerido para searxng)
    - frontend
    - backend (si usas `full` para la interfaz web)

```sh
./start_services.sh full # MacOS
start start_services.cmd full # Windows
```

**Advertencia:** Este paso descargar√° y cargar√° todas las im√°genes de Docker, lo que puede tardar hasta 30 minutos. Despu√©s de iniciar los servicios, espera hasta que el servicio backend est√© completamente ejecut√°ndose (deber√≠as ver **backend: "GET /health HTTP/1.1" 200 OK** en el registro) antes de enviar cualquier mensaje. En la primera ejecuci√≥n, el servicio backend puede tardar 5 minutos en iniciarse.

Ve a `http://localhost:3000/` y deber√≠as ver la interfaz web.

*Soluci√≥n de problemas de inicio de servicios:* Si estos scripts fallan, aseg√∫rate de que Docker Engine est√© ejecut√°ndose y que Docker Compose (V2, `docker compose`) est√© correctamente instalado. Revisa los mensajes de error en la salida de la terminal. Consulta [FAQ: ¬°Ayuda! Obtengo errores al ejecutar AgenticSeek o sus scripts.](#faq-soluci√≥n-de-problemas)

**Opci√≥n 2:** Modo CLI:

Para ejecutar con la interfaz CLI, debes instalar los paquetes en el host:

```sh
./install.sh
./install.bat # windows
```

Luego debes cambiar SEARXNG_BASE_URL en `config.ini` a:

```sh
SEARXNG_BASE_URL="http://localhost:8080"
```

Inicia los servicios necesarios. Esto iniciar√° algunos servicios del docker-compose.yml, incluyendo:
    - searxng
    - redis (requerido para searxng)
    - frontend

```sh
./start_services.sh # MacOS
start start_services.cmd # Windows
```

Ejecuta: uv run: `uv run python -m ensurepip` para asegurarte de que uv tenga pip habilitado.

Usa CLI: `uv run cli.py`

---

## Uso

Aseg√∫rate de que los servicios est√©n ejecut√°ndose con `./start_services.sh full` y luego ve a `localhost:3000` para la interfaz web.

Tambi√©n puedes usar voz a texto configurando `listen = True`. Solo para modo CLI.

Para salir, simplemente di/escribe `goodbye`.

Algunos ejemplos de uso:

> *¬°Haz un juego de la serpiente en python!*

> *Busca en la web los mejores caf√©s en Rennes, Francia, y guarda una lista de tres con sus direcciones en rennes_cafes.txt.*

> *Escribe un programa Go para calcular el factorial de un n√∫mero, gu√°rdalo como factorial.go en tu workspace*

> *Busca en la carpeta summer_pictures todos los archivos JPG, ren√≥mbralos con la fecha de hoy y guarda la lista de archivos renombrados en photos_list.txt*

> *Busca en l√≠nea pel√≠culas de ciencia ficci√≥n populares de 2024 y elige tres para ver esta noche. Guarda la lista en movie_night.txt.*

> *Busca en la web los √∫ltimos art√≠culos de noticias de IA de 2025, selecciona tres y escribe un script Python para extraer t√≠tulos y res√∫menes. Guarda el script como news_scraper.py y los res√∫menes en ai_news.txt en /home/projects*

> *Viernes, busca en la web una API gratuita de precios de acciones, reg√≠strate con supersuper7434567@gmail.com y escribe un script Python para obtener los precios diarios de Tesla usando la API, guardando los resultados en stock_prices.csv*

*Ten en cuenta que el llenado de formularios sigue siendo experimental y puede fallar.*

Despu√©s de ingresar tu consulta, AgenticSeek asignar√° el mejor agente para la tarea.

Como este es un prototipo inicial, el sistema de enrutamiento de agentes puede no asignar siempre el agente correcto a tu consulta.

Por lo tanto, s√© muy expl√≠cito sobre lo que quieres y c√≥mo la IA podr√≠a proceder, por ejemplo, si quieres que realice una b√∫squeda web, no digas:

`¬øConoces algunos buenos pa√≠ses para viajar solo?`

En su lugar, di:

`Realiza una b√∫squeda web y descubre cu√°les son los mejores pa√≠ses para viajar solo`

---

## **Configuraci√≥n para ejecutar LLM en tu propio servidor**

Si tienes una computadora potente o un servidor al que puedes acceder, pero quieres usarlo desde tu laptop, puedes optar por ejecutar el LLM en un servidor remoto usando nuestro servidor llm personalizado.

En tu "servidor" que ejecutar√° el modelo de IA, obt√©n la direcci√≥n IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # IP local
curl https://ipinfo.io/ip # IP p√∫blica
```

Nota: Para Windows o macOS, usa ipconfig o ifconfig para encontrar la direcci√≥n IP.

Clona el repositorio y entra en la carpeta `server/`.

```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Instala los requisitos espec√≠ficos del servidor:

```sh
pip3 install -r requirements.txt
```

Ejecuta el script del servidor.

```sh
python3 app.py --provider ollama --port 3333
```

Puedes elegir entre usar `ollama` y `llamacpp` como servicio LLM.

Ahora en tu computadora personal:

Cambia el archivo `config.ini` para establecer `provider_name` como `server` y `provider_model` como `deepseek-r1:xxb`.
Establece `provider_server_address` a la direcci√≥n IP de la m√°quina que ejecutar√° el modelo.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333
```

Siguiente paso: [Iniciar servicios y ejecutar AgenticSeek](#iniciar-servicios-y-ejecutar)  

---

## Voz a Texto

Advertencia: La voz a texto solo funciona en modo CLI en este momento.

Ten en cuenta que la voz a texto solo funciona en ingl√©s en este momento.

La funcionalidad de voz a texto est√° deshabilitada por defecto. Para habilitarla, establece listen en True en el archivo config.ini:

```
listen = True
```

Cuando est√° habilitado, la funci√≥n de voz a texto escucha una palabra clave de activaci√≥n, que es el nombre del agente, antes de procesar tu entrada. Puedes personalizar el nombre del agente actualizando el valor `agent_name` en *config.ini*:

```
agent_name = Friday
```

Para un mejor reconocimiento, recomendamos usar un nombre com√∫n en ingl√©s como "John" o "Emma" como nombre de agente.

Una vez que veas que comienza a aparecer la transcripci√≥n, di el nombre del agente en voz alta para activarlo (ej: "Friday").

Di tu consulta claramente.

Termina tu solicitud con una frase de confirmaci√≥n para indicar al sistema que proceda. Ejemplos de frases de confirmaci√≥n incluyen:
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Configuraci√≥n

Ejemplo de configuraci√≥n:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Ejemplo de Ollama; LM-Studio usa http://127.0.0.1:1234
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Lista de idiomas para TTS y enrutamiento potencial.
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explicaci√≥n de la configuraci√≥n de `config.ini`**:

*   **Secci√≥n `[MAIN]`:**
    *   `is_local`: `True` si usas proveedores de LLM locales (Ollama, LM-Studio, servidor local compatible con OpenAI) o la opci√≥n de servidor autoalojado. `False` si usas API basadas en la nube (OpenAI, Google, etc.).
    *   `provider_name`: Especifica el proveedor de LLM.
        *   Opciones locales: `ollama`, `lm-studio`, `openai` (para servidor local compatible con OpenAI), `server` (para configuraci√≥n de servidor autoalojado).
        *   Opciones de API: `openai`, `google`, `deepseek`, `huggingface`, `togetherAI`.
    *   `provider_model`: Nombre o ID espec√≠fico del modelo del proveedor seleccionado (ej: `deepseekcoder:6.7b` para Ollama, `gpt-3.5-turbo` para API de OpenAI, `mistralai/Mixtral-8x7B-Instruct-v0.1` para TogetherAI).
    *   `provider_server_address`: La direcci√≥n de tu proveedor de LLM.
        *   Para proveedores locales: ej: `http://127.0.0.1:11434` para Ollama, `http://127.0.0.1:1234` para LM-Studio.
        *   Para el tipo de proveedor `server`: La direcci√≥n de tu servidor LLM autoalojado (ej: `http://your_server_ip:3333`).
        *   Para API en la nube (`is_local = False`): Esto generalmente se ignora o puede dejarse en blanco, ya que los endpoints de API suelen ser manejados por las bibliotecas del cliente.
    *   `agent_name`: El nombre del asistente de IA (ej: Friday). Si est√° habilitado, se utiliza como palabra de activaci√≥n para voz a texto.
    *   `recover_last_session`: `True` para intentar recuperar el estado de la sesi√≥n anterior, `False` para comenzar de nuevo.
    *   `save_session`: `True` para guardar el estado de la sesi√≥n actual para una posible recuperaci√≥n, `False` en caso contrario.
    *   `speak`: `True` para habilitar la salida de voz de texto a voz, `False` para deshabilitar.
    *   `listen`: `True` para habilitar la entrada de voz de voz a texto (solo modo CLI), `False` para deshabilitar.
    *   `work_dir`: **Cr√≠tico:** El directorio donde AgenticSeek leer√°/escribir√° archivos. **Aseg√∫rate de que esta ruta sea v√°lida y accesible en tu sistema.**
    *   `jarvis_personality`: `True` para usar prompts del sistema m√°s al estilo "Jarvis" (experimental), `False` para usar prompts est√°ndar.
    *   `languages`: Lista de idiomas separados por comas (ej: `en, zh, fr`). Se utiliza para la selecci√≥n de voz TTS (predeterminado el primero) y puede ayudar al enrutador LLM. Para evitar ineficiencias del enrutador, evita usar demasiados idiomas o idiomas muy similares.
*   **Secci√≥n `[BROWSER]`:**
    *   `headless_browser`: `True` para ejecutar el navegador automatizado sin una ventana visible (recomendado para interfaz web o uso no interactivo). `False` para mostrar la ventana del navegador (√∫til para modo CLI o depuraci√≥n).
    *   `stealth_mode`: `True` para habilitar medidas que dificultan la detecci√≥n de la automatizaci√≥n del navegador. Puede requerir la instalaci√≥n manual de extensiones del navegador como anticaptcha.

Esta secci√≥n resume los tipos de proveedores de LLM admitidos. Config√∫ralos en `config.ini`.

**Proveedores locales (ejecut√°ndose en tu propio hardware):**

| Nombre del proveedor en config.ini | `is_local` | Descripci√≥n                                                                 | Secci√≥n de configuraci√≥n                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|------------------------------------------------------------------|
| `ollama`                      | `True`     | Proporciona LLM localmente f√°cilmente usando Ollama.                                             | [Configuraci√≥n para ejecutar LLM localmente en tu m√°quina](#configuraci√≥n-para-ejecutar-llm-localmente-en-tu-m√°quina) |
| `lm-studio`                   | `True`     | Proporciona LLM localmente con LM-Studio.                                          | [Configuraci√≥n para ejecutar LLM localmente en tu m√°quina](#configuraci√≥n-para-ejecutar-llm-localmente-en-tu-m√°quina) |
| `openai` (para servidor local)   | `True`     | Con√©ctate a un servidor local que exponga una API compatible con OpenAI (ej: llama.cpp). | [Configuraci√≥n para ejecutar LLM localmente en tu m√°quina](#configuraci√≥n-para-ejecutar-llm-localmente-en-tu-m√°quina) |
| `server`                      | `False`    | Con√©ctate al servidor LLM autoalojado de AgenticSeek que se ejecuta en otra m√°quina. | [Configuraci√≥n para ejecutar LLM en tu propio servidor](#configuraci√≥n-para-ejecutar-llm-en-tu-propio-servidor) |

**Proveedores de API (basados en la nube):**

| Nombre del proveedor en config.ini | `is_local` | Descripci√≥n                                      | Secci√≥n de configuraci√≥n                                       |
|-------------------------------|------------|--------------------------------------------------|-----------------------------------------------------|
| `openai`                      | `False`    | Usa la API oficial de OpenAI (ej: GPT-3.5, GPT-4). | [Configuraci√≥n para ejecutar con una API](#configuraci√≥n-para-ejecutar-con-una-api) |
| `google`                      | `False`    | Usa modelos Google Gemini a trav√©s de API.              | [Configuraci√≥n para ejecutar con una API](#configuraci√≥n-para-ejecutar-con-una-api) |
| `deepseek`                    | `False`    | Usa la API oficial de Deepseek.                     | [Configuraci√≥n para ejecutar con una API](#configuraci√≥n-para-ejecutar-con-una-api) |
| `huggingface`                 | `False`    | Usa Hugging Face Inference API.                  | [Configuraci√≥n para ejecutar con una API](#configuraci√≥n-para-ejecutar-con-una-api) |
| `togetherAI`                  | `False`    | Usa varios modelos abiertos a trav√©s de la API de TogetherAI.    | [Configuraci√≥n para ejecutar con una API](#configuraci√≥n-para-ejecutar-con-una-api) |

---
## Soluci√≥n de problemas

Si encuentras problemas, esta secci√≥n proporciona orientaci√≥n.

# Problemas conocidos

## Problemas de ChromeDriver

**Ejemplo de error:** `SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX`

### Causa principal
La incompatibilidad de versi√≥n de ChromeDriver ocurre cuando:
1. La versi√≥n de ChromeDriver que instalaste no coincide con la versi√≥n del navegador Chrome
2. En entornos Docker, `undetected_chromedriver` puede descargar su propia versi√≥n de ChromeDriver, evitando los binarios montados

### Pasos de soluci√≥n

#### 1. Verifica tu versi√≥n de Chrome
Abre Google Chrome ‚Üí `Configuraci√≥n > Acerca de Chrome` para encontrar tu versi√≥n (ej: "Versi√≥n 134.0.6998.88")

#### 2. Descarga ChromeDriver coincidente

**Para Chrome 115 y versiones m√°s recientes:** Usa [Chrome for Testing API](https://googlechromelabs.github.io/chrome-for-testing/)
- Visita el panel de disponibilidad de Chrome for Testing
- Encuentra tu versi√≥n de Chrome o la coincidencia disponible m√°s cercana
- Descarga ChromeDriver para tu sistema operativo (usa Linux64 para entornos Docker)

**Para versiones antiguas de Chrome:** Usa [Descargas heredadas de ChromeDriver](https://chromedriver.chromium.org/downloads)

![Descargar ChromeDriver desde Chrome for Testing](./media/chromedriver_readme.png)

#### 3. Instala ChromeDriver (elige un m√©todo)

**M√©todo A: Directorio ra√≠z del proyecto (recomendado para Docker)**
```bash
# Coloca el binario de chromedriver descargado en el directorio ra√≠z del proyecto
cp path/to/downloaded/chromedriver ./chromedriver
chmod +x ./chromedriver  # Hazlo ejecutable en Linux/macOS
```

**M√©todo B: PATH del sistema**
```bash
# Linux/macOS
sudo mv chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver

# Windows: Coloca chromedriver.exe en una carpeta en PATH
```

#### 4. Verifica la instalaci√≥n
```bash
# Prueba la versi√≥n de ChromeDriver
./chromedriver --version
# O si est√° en PATH:
chromedriver --version
```

### Instrucciones espec√≠ficas de Docker

‚ö†Ô∏è **Importante para usuarios de Docker:**
- El m√©todo de montaje de vol√∫menes de Docker puede no funcionar con el modo sigiloso (`undetected_chromedriver`)
- **Soluci√≥n:** Coloca ChromeDriver en el directorio ra√≠z del proyecto como `./chromedriver`
- La aplicaci√≥n lo detectar√° autom√°ticamente y usar√° este binario
- Deber√≠as ver en los registros: `"Using ChromeDriver from project root: ./chromedriver"`

### Consejos para soluci√≥n de problemas

1. **¬øSigues teniendo incompatibilidad de versi√≥n?**
   - Verifica que ChromeDriver sea ejecutable: `ls -la ./chromedriver`
   - Comprueba la versi√≥n de ChromeDriver: `./chromedriver --version`
   - Aseg√∫rate de que coincida con tu versi√≥n del navegador Chrome

2. **¬øProblemas con el contenedor Docker?**
   - Revisa los registros del backend: `docker logs backend`
   - Busca el mensaje: `"Using ChromeDriver from project root"`
   - Si no se encuentra, verifica que el archivo exista y sea ejecutable

3. **Versiones de Chrome for Testing**
   - Usa una coincidencia exacta cuando sea posible
   - Para la versi√≥n 134.0.6998.88, usa ChromeDriver 134.0.6998.165 (la versi√≥n disponible m√°s cercana)
   - El n√∫mero de versi√≥n principal debe coincidir (134 = 134)

### Matriz de compatibilidad de versiones

| Versi√≥n de Chrome | Versi√≥n de ChromeDriver | Estado |
|----------------|---------------------|---------|
| 134.0.6998.x   | 134.0.6998.165     | ‚úÖ Disponible |
| 133.0.6943.x   | 133.0.6943.141     | ‚úÖ Disponible |
| 132.0.6834.x   | 132.0.6834.159     | ‚úÖ Disponible |

*Para la compatibilidad m√°s reciente, consulta el [Panel de Chrome for Testing](https://googlechromelabs.github.io/chrome-for-testing/)*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Esto sucede si tu navegador y la versi√≥n de chromedriver no coinciden.

Necesitas navegar para descargar la versi√≥n m√°s reciente:

https://developer.chrome.com/docs/chromedriver/downloads

Si usas Chrome versi√≥n 115 o superior, ve a:

https://googlechromelabs.github.io/chrome-for-testing/

y descarga la versi√≥n de chromedriver que coincida con tu sistema operativo.

![alt text](./media/chromedriver_readme.png)

Si esta secci√≥n est√° incompleta, abre un issue.

##  Problemas de adaptadores de conexi√≥n

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (nota: el puerto puede variar)
```

*   **Causa:** Falta el prefijo `http://` en `provider_server_address` para `lm-studio` (u otro servidor local compatible con OpenAI similar) en `config.ini`, o apunta al puerto incorrecto.
*   **Soluci√≥n:**
    *   Aseg√∫rate de que la direcci√≥n incluya `http://`. LM-Studio normalmente usa `http://127.0.0.1:1234` por defecto.
    *   `config.ini` correcto: `provider_server_address = http://127.0.0.1:1234` (o tu puerto real del servidor LM-Studio).

## URL base de SearxNG no proporcionada

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
```

Esto puede ocurrir si ejecutas el modo CLI con la URL base de searxng incorrecta.

SEARXNG_BASE_URL debe diferir seg√∫n si ejecutas en Docker o en el host:

**Ejecutando en el host:** `SEARXNG_BASE_URL="http://localhost:8080"`

**Ejecutando completamente en Docker (interfaz web):** `SEARXNG_BASE_URL="http://searxng:8080"`

## FAQ

**P: ¬øQu√© hardware necesito?**  

| Tama√±o del modelo  | GPU  | Comentarios                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB VRAM | ‚ö†Ô∏è No recomendado. Rendimiento pobre, alucinaciones frecuentes, los agentes de planificaci√≥n pueden fallar. |
| 14B        | 12 GB VRAM (ej: RTX 3060) | ‚úÖ Utilizable para tareas simples. Puede tener dificultades con la navegaci√≥n web y la planificaci√≥n de tareas. |
| 32B        | 24+ GB VRAM (ej: RTX 4090) | üöÄ √âxito en la mayor√≠a de las tareas, a√∫n puede tener dificultades con la planificaci√≥n de tareas |
| 70B+        | 48+ GB VRAM | üí™ Excelente. Recomendado para casos de uso avanzados. |

**P: ¬øQu√© hago si encuentro errores?**  

Aseg√∫rate de que lo local est√© ejecut√°ndose (`ollama serve`), que tu `config.ini` coincida con tu proveedor y que las dependencias est√©n instaladas. Si nada funciona, no dudes en abrir un issue.

**P: ¬øRealmente puede ejecutarse 100% localmente?**  

S√≠, con proveedores Ollama, lm-studio o server, todos los modelos de voz a texto, LLM y texto a voz se ejecutan localmente. Las opciones no locales (OpenAI u otras API) son opcionales.

**P: ¬øPor qu√© deber√≠a usar AgenticSeek cuando tengo Manus?**

A diferencia de Manus, AgenticSeek prioriza la independencia de los sistemas externos, d√°ndote m√°s control, privacidad y evitando costos de API.

**P: ¬øQui√©n est√° detr√°s de este proyecto?**

Este proyecto fue creado por m√≠, con dos amigos como mantenedores y contribuyentes de la comunidad de c√≥digo abierto en GitHub. Solo somos individuos apasionados, no una startup, ni estamos afiliados a ninguna organizaci√≥n.

Cualquier cuenta de AgenticSeek en X adem√°s de mi cuenta personal (https://x.com/Martin993886460) es impostora.

## Contribuir

¬°Buscamos desarrolladores para mejorar AgenticSeek! Revisa los issues abiertos o discusiones.

[Gu√≠a de contribuci√≥n](./docs/CONTRIBUTING.md)

## Patrocinadores:

¬øQuieres mejorar las capacidades de AgenticSeek con funciones como b√∫squeda de vuelos, planificaci√≥n de viajes o obtenci√≥n de las mejores ofertas de compras? Considera usar SerpApi para crear herramientas personalizadas que desbloqueen m√°s funcionalidades al estilo Jarvis. Con SerpApi, puedes acelerar tu agente para tareas profesionales mientras mantienes el control total.

<a href="https://serpapi.com/"><img src="./media/banners/sponsor_banner_serpapi.png" height="350" alt="SerpApi Banner" ></a>

¬°Consulta [Contributing.md](./docs/CONTRIBUTING.md) para aprender c√≥mo integrar herramientas personalizadas!

### **Patrocinadores**:

- [tatra-labs](https://github.com/tatra-labs)

## Mantenedores:

 > [Fosowl](https://github.com/Fosowl) | Hora de Par√≠s 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Hora de Taipei 

## Agradecimientos especiales:

 > [tcsenpai](https://github.com/tcsenpai) y [plitc](https://github.com/plitc) por ayudar con la dockerizaci√≥n del backend

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)
