# AgenticSeek : Une Alternative Priv√©e et Locale √† Manus

<p align="center">
<img align="center" src="./media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

  English | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | [Fran√ßais](./README_FR.md) | [Êó•Êú¨Ë™û](./README_JP.md) | [Portugu√™s (Brasil)](./README_PTBR.md) | [Espa√±ol](./README_ES.md)

*Un assistant IA avec reconnaissance vocale qui est une **alternative 100% locale √† Manus AI**, navigue de mani√®re autonome sur le web, √©crit du code et planifie des t√¢ches tout en gardant toutes les donn√©es sur votre appareil. Con√ßu pour des mod√®les de raisonnement locaux, il fonctionne enti√®rement sur votre mat√©riel, garantissant une confidentialit√© totale et z√©ro d√©pendance au cloud.*

[![Visiter AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Pourquoi choisir AgenticSeek ?

* üîí Totalement Local & Priv√© - Tout fonctionne sur votre machine, sans cloud, sans partage de donn√©es. Vos fichiers, conversations et recherches restent priv√©s.

* üåê Navigation Web Intelligente - AgenticSeek peut naviguer sur Internet de mani√®re autonome : rechercher, lire, extraire des informations, remplir des formulaires web, le tout sans intervention manuelle.

* üíª Assistant de Programmation Autonome - Besoin de code ? Il peut √©crire, d√©boguer et ex√©cuter des programmes en Python, C, Go, Java et plus encore, sans supervision.

* üß† S√©lection Intelligente d'Agents - Vous demandez, il choisit automatiquement le meilleur agent pour la t√¢che. Comme avoir une √©quipe d'experts toujours disponible.

* üìã Planifie et Ex√©cute des T√¢ches Complexes - De la planification de voyage aux projets complexes, il peut d√©composer de grandes t√¢ches en √©tapes et les compl√©ter en utilisant plusieurs agents IA.

* üéôÔ∏è Prise en Charge Vocale - Voix claire, rapide et futuriste avec reconnaissance vocale, vous permettant de converser comme avec votre IA personnelle de film de science-fiction. (En d√©veloppement)

### **D√©mo**

> *Peux-tu rechercher le projet agenticSeek, apprendre quelles comp√©tences sont n√©cessaires, puis ouvrir CV_candidates.zip et me dire lesquels correspondent le mieux au projet ?*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Avertissement : Cette d√©monstration et tous les fichiers qui apparaissent (ex: CV_candidates.zip) sont enti√®rement fictifs. Nous ne sommes pas une entreprise, nous recherchons des contributeurs open source, pas des candidats.

> üõ†‚ö†Ô∏èÔ∏è **Travail Actif en Cours**

> üôè Ce projet a commenc√© comme un projet parall√®le et n'a ni feuille de route ni financement. Il a grandi bien au-del√† des attentes en apparaissant dans GitHub Trending. Les contributions, commentaires et de la patience sont profond√©ment appr√©ci√©s.

## Pr√©requis

Avant de commencer, assurez-vous d'avoir install√© :

*   **Git:** Pour cloner le d√©p√¥t. [T√©l√©charger Git](https://git-scm.com/downloads)
*   **Python 3.10.x:** Python 3.10.x est fortement recommand√©. D'autres versions peuvent causer des erreurs de d√©pendance. [T√©l√©charger Python 3.10](https://www.python.org/downloads/release/python-3100/) (s√©lectionnez la version 3.10.x).
*   **Docker Engine & Docker Compose:** Pour ex√©cuter des services empaquet√©s comme SearxNG.
    *   Installer Docker Desktop (inclut Docker Compose V2): [Windows](https://docs.docker.com/desktop/install/windows-install/) | [Mac](https://docs.docker.com/desktop/install/mac-install/) | [Linux](https://docs.docker.com/desktop/install/linux-install/)
    *   Ou installer Docker Engine et Docker Compose s√©par√©ment sur Linux: [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/) (assurez-vous d'installer Compose V2, par exemple `sudo apt-get install docker-compose-plugin`).

### 1. **Cloner le d√©p√¥t et configurer**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Modifier le contenu du fichier .env

```sh
SEARXNG_BASE_URL="http://searxng:8080" # Si vous ex√©cutez en mode CLI sur l'h√¥te, utilisez http://127.0.0.1:8080
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
```

Mettez √† jour le fichier `.env` selon vos besoins :

- **SEARXNG_BASE_URL**: Gardez inchang√© sauf si vous ex√©cutez en mode CLI sur l'h√¥te.
- **REDIS_BASE_URL**: Gardez inchang√© 
- **WORK_DIR**: Chemin vers le r√©pertoire de travail local. AgenticSeek pourra lire et interagir avec ces fichiers.
- **OLLAMA_PORT**: Num√©ro de port pour le service Ollama.
- **LM_STUDIO_PORT**: Num√©ro de port pour le service LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Port pour tout service LLM personnalis√© suppl√©mentaire.

**Les cl√©s API sont compl√®tement optionnelles pour ceux qui choisissent d'ex√©cuter LLM localement, ce qui est l'objectif principal de ce projet. Laissez-les vides si vous avez du mat√©riel suffisant.**

### 3. **D√©marrer Docker**

Assurez-vous que Docker est install√© et fonctionne sur votre syst√®me. Vous pouvez d√©marrer Docker avec les commandes suivantes :

- **Linux/macOS:**  
    Ouvrez un terminal et ex√©cutez :
    ```sh
    sudo systemctl start docker
    ```
    Ou d√©marrez Docker Desktop depuis le menu des applications, s'il est install√©.

- **Windows:**  
    D√©marrez Docker Desktop depuis le menu D√©marrer.

Vous pouvez v√©rifier si Docker fonctionne en ex√©cutant :
```sh
docker info
```
Si vous voyez des informations sur votre installation Docker, cela fonctionne correctement.

Consultez la [Liste des fournisseurs locaux](#liste-des-fournisseurs-locaux) ci-dessous pour un r√©sum√©.

Prochaine √©tape: [Ex√©cuter AgenticSeek localement](#d√©marrer-les-services-et-ex√©cuter)

*Si vous rencontrez des probl√®mes, consultez la section [D√©pannage](#d√©pannage).*
*Si votre mat√©riel ne peut pas ex√©cuter LLM localement, consultez [Configuration pour ex√©cuter avec une API](#configuration-pour-ex√©cuter-avec-une-api).*
*Pour des explications d√©taill√©es de `config.ini`, consultez la [section Configuration](#configuration).*

---

## Configuration pour ex√©cuter LLM localement sur votre machine

**Exigences mat√©rielles:**

Pour ex√©cuter LLM localement, vous aurez besoin de mat√©riel suffisant. Au minimum, une GPU capable d'ex√©cuter Magistral, Qwen ou Deepseek 14B est requise. Consultez la FAQ pour des recommandations d√©taill√©es de mod√®le/performance.

**Configurez votre fournisseur local**  

D√©marrez votre fournisseur local, par exemple avec ollama:

```sh
ollama serve
```

Consultez la liste des fournisseurs locaux pris en charge ci-dessous.

**Mettre √† jour config.ini**

Changez le fichier config.ini pour d√©finir provider_name sur un fournisseur pris en charge et provider_model sur un LLM pris en charge par votre fournisseur. Nous recommandons des mod√®les de raisonnement comme *Magistral* ou *Deepseek*.

Consultez la **FAQ** √† la fin du README pour le mat√©riel n√©cessaire.

```sh
[MAIN]
is_local = True # Que vous ex√©cutiez localement ou avec un fournisseur distant.
provider_name = ollama # ou lm-studio, openai, etc.
provider_model = deepseek-r1:14b # choisissez un mod√®le compatible avec votre mat√©riel
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # le nom de votre IA
recover_last_session = True # r√©cup√©rer la session pr√©c√©dente
save_session = True # m√©moriser la session actuelle
speak = False # texte vers parole
listen = False # parole vers texte, uniquement pour CLI, exp√©rimental
jarvis_personality = False # utiliser une personnalit√© plus "Jarvis" (exp√©rimental)
languages = en zh # Liste des langues, TTS utilisera la premi√®re de la liste par d√©faut
[BROWSER]
headless_browser = True # garder inchang√© sauf si vous utilisez CLI sur l'h√¥te.
stealth_mode = True # Utilise selenium ind√©tectable pour r√©duire la d√©tection du navigateur
```

**Avertissement**:

- Le format du fichier `config.ini` ne prend pas en charge les commentaires.
Ne copiez et collez pas directement la configuration d'exemple, car les commentaires causeront des erreurs. Modifiez plut√¥t manuellement le fichier `config.ini` avec votre configuration souhait√©e, sans commentaires.

- *NE* d√©finissez PAS provider_name sur `openai` si vous utilisez LM-studio pour ex√©cuter LLM. Utilisez `lm-studio`.

- Certains fournisseurs (ex: lm-studio) n√©cessitent `http://` avant l'IP. Exemple: `http://127.0.0.1:1234`

**Liste des fournisseurs locaux**

| Fournisseur  | Local ? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Oui    | Ex√©cute LLM localement facilement en utilisant ollama |
| lm-studio  | Oui    | Ex√©cute LLM localement avec LM studio (d√©finir `provider_name` = `lm-studio`)|
| openai    | Oui     |  Utilise une API compatible avec openai (ex: serveur llama.cpp)  |

Prochaine √©tape: [D√©marrer les services et ex√©cuter AgenticSeek](#d√©marrer-les-services-et-ex√©cuter)  

*Si vous rencontrez des probl√®mes, consultez la section [D√©pannage](#d√©pannage).*
*Si votre mat√©riel ne peut pas ex√©cuter LLM localement, consultez [Configuration pour ex√©cuter avec une API](#configuration-pour-ex√©cuter-avec-une-api).*
*Pour des explications d√©taill√©es de `config.ini`, consultez la [section Configuration](#configuration).*

## Configuration pour ex√©cuter avec une API

Cette configuration utilise des fournisseurs de LLM externes bas√©s sur le cloud. Vous devrez obtenir des cl√©s API du service choisi.

**1. Choisissez un fournisseur d'API et obtenez une cl√© API:**

Consultez la [Liste des fournisseurs d'API](#liste-des-fournisseurs-dapi) ci-dessous. Visitez leurs sites web pour vous inscrire et obtenir des cl√©s API.

**2. D√©finissez votre cl√© API comme variable d'environnement:**

*   **Linux/macOS:**
    Ouvrez un terminal et utilisez la commande `export`. Il est pr√©f√©rable de l'ajouter au fichier de configuration de votre shell (ex: `~/.bashrc`, `~/.zshrc`) pour qu'elle soit persistante.
    ```sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # Remplacez PROVIDER_API_KEY par le nom de variable sp√©cifique, ex: OPENAI_API_KEY, GOOGLE_API_KEY
    ```
    Exemple TogetherAI:
    ```sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    ```
*   **Windows:**
    *   **Invite de commandes (temporaire pour la session actuelle):**
        ```cmd
        set PROVIDER_API_KEY=your_api_key_here
        ```
    *   **PowerShell (temporaire pour la session actuelle):**
        ```powershell
        $env:PROVIDER_API_KEY="your_api_key_here"
        ```
    *   **Permanent:** Recherchez "variables d'environnement" dans la barre de recherche Windows, cliquez sur "Modifier les variables d'environnement syst√®me", puis sur le bouton "Variables d'environnement...". Ajoutez une nouvelle variable utilisateur avec le nom appropri√© (ex: `OPENAI_API_KEY`) et votre cl√© comme valeur.

    *(Pour plus de d√©tails, consultez la FAQ: [Comment configurer une cl√© API ?](#comment-configurer-une-cl√©-api)).*


**3. Mettez √† jour `config.ini`:**
```ini
[MAIN]
is_local = False
provider_name = openai # ou google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1, etc.
provider_server_address = # Lorsque is_local = False, g√©n√©ralement ignor√© ou peut √™tre laiss√© vide pour la plupart des API
# ... autres configurations ...
```
*Avertissement:* Assurez-vous qu'il n'y a pas d'espaces √† la fin des valeurs dans config.

**Liste des fournisseurs d'API**

| Fournisseur     | `provider_name` | Local ? | Description                                       | Lien de cl√© API (exemple)                     |
|--------------|-----------------|--------|---------------------------------------------------|---------------------------------------------|
| OpenAI       | `openai`        | Non     | Utilise les mod√®les ChatGPT via l'API OpenAI.              | [platform.openai.com/signup](https://platform.openai.com/signup) |
| Google Gemini| `google`        | Non     | Utilise les mod√®les Google Gemini via Google AI Studio.    | [aistudio.google.com/keys](https://aistudio.google.com/keys) |
| Deepseek     | `deepseek`      | Non     | Utilise les mod√®les Deepseek via leur API.                | [platform.deepseek.com](https://platform.deepseek.com) |
| Hugging Face | `huggingface`   | Non     | Utilise les mod√®les du Hugging Face Inference API.       | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |
| TogetherAI   | `togetherAI`    | Non     | Utilise divers mod√®les open source via l'API TogetherAI.| [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) |

*Note:*
*   Nous ne recommandons pas d'utiliser `gpt-4o` ou d'autres mod√®les OpenAI pour la navigation web complexe et la planification de t√¢ches, car l'optimisation actuelle des prompts cible des mod√®les comme Deepseek.
*   Les t√¢ches de codage/bash peuvent √©chouer avec Gemini, car il a tendance √† ignorer notre format de prompt optimis√© pour Deepseek r1.
*   Lorsque `is_local = False`, `provider_server_address` dans `config.ini` n'est g√©n√©ralement pas utilis√©, car les endpoints d'API sont g√©n√©ralement g√©r√©s par les biblioth√®ques du fournisseur correspondant.

Prochaine √©tape: [D√©marrer les services et ex√©cuter AgenticSeek](#d√©marrer-les-services-et-ex√©cuter)

*Si vous rencontrez des probl√®mes, consultez la section **Probl√®mes connus***

*Pour des explications d√©taill√©es du fichier de configuration, consultez la **section Configuration**.*

---

## D√©marrer les services et ex√©cuter

Par d√©faut, AgenticSeek s'ex√©cute enti√®rement dans Docker.

**Option 1:** Ex√©cuter dans Docker avec interface web:

D√©marrez les services n√©cessaires. Cela d√©marrera tous les services du docker-compose.yml, y compris:
    - searxng
    - redis (requis pour searxng)
    - frontend
    - backend (si vous utilisez `full` pour l'interface web)

```sh
./start_services.sh full # MacOS
start start_services.cmd full # Windows
```

**Avertissement:** Cette √©tape t√©l√©chargera et chargera toutes les images Docker, ce qui peut prendre jusqu'√† 30 minutes. Apr√®s avoir d√©marr√© les services, attendez que le service backend soit compl√®tement op√©rationnel (vous devriez voir **backend: "GET /health HTTP/1.1" 200 OK** dans les logs) avant d'envoyer des messages. Lors du premier d√©marrage, le service backend peut prendre 5 minutes pour d√©marrer.

Allez √† `http://localhost:3000/` et vous devriez voir l'interface web.

*D√©pannage du d√©marrage des services:* Si ces scripts √©chouent, assurez-vous que Docker Engine fonctionne et que Docker Compose (V2, `docker compose`) est correctement install√©. V√©rifiez les messages d'erreur dans la sortie du terminal. Consultez [FAQ: Aide ! J'obtiens des erreurs lors de l'ex√©cution d'AgenticSeek ou de ses scripts.](#faq-d√©pannage)

**Option 2:** Mode CLI:

Pour ex√©cuter avec l'interface CLI, vous devez installer les packages sur l'h√¥te:

```sh
./install.sh
./install.bat # windows
```

Ensuite, vous devez changer SEARXNG_BASE_URL dans `config.ini` en:

```sh
SEARXNG_BASE_URL="http://localhost:8080"
```

D√©marrez les services n√©cessaires. Cela d√©marrera certains services du docker-compose.yml, y compris:
    - searxng
    - redis (requis pour searxng)
    - frontend

```sh
./start_services.sh # MacOS
start start_services.cmd # Windows
```

Ex√©cutez: uv run: `uv run python -m ensurepip` pour vous assurer que uv a pip activ√©.

Utilisez CLI: `uv run cli.py`

---

## Utilisation

Assurez-vous que les services fonctionnent avec `./start_services.sh full` puis allez √† `localhost:3000` pour l'interface web.

Vous pouvez √©galement utiliser la parole vers texte en d√©finissant `listen = True`. Uniquement pour le mode CLI.

Pour quitter, dites/tapez simplement `goodbye`.

Quelques exemples d'utilisation:

> *Fais un jeu de serpent en python !*

> *Recherche sur le web les meilleurs caf√©s √† Rennes, France, et sauvegarde une liste de trois avec leurs adresses dans rennes_cafes.txt.*

> *√âcris un programme Go pour calculer la factorielle d'un nombre, sauvegarde-le comme factorial.go dans ton workspace*

> *Recherche dans le dossier summer_pictures tous les fichiers JPG, renomme-les avec la date d'aujourd'hui et sauvegarde la liste des fichiers renomm√©s dans photos_list.txt*

> *Recherche en ligne les films de science-fiction populaires de 2024 et choisis-en trois √† regarder ce soir. Sauvegarde la liste dans movie_night.txt.*

> *Recherche sur le web les derniers articles d'actualit√© sur l'IA de 2025, s√©lectionne-en trois et √©cris un script Python pour extraire les titres et r√©sum√©s. Sauvegarde le script comme news_scraper.py et les r√©sum√©s dans ai_news.txt dans /home/projects*

> *Vendredi, recherche sur le web une API gratuite de prix d'actions, inscris-toi avec supersuper7434567@gmail.com et √©cris un script Python pour obtenir les prix quotidiens de Tesla en utilisant l'API, en sauvegardant les r√©sultats dans stock_prices.csv*

*Notez que le remplissage de formulaires est toujours exp√©rimental et peut √©chouer.*

Apr√®s avoir saisi votre requ√™te, AgenticSeek attribuera le meilleur agent pour la t√¢che.

Comme il s'agit d'un prototype initial, le syst√®me de routage des agents peut ne pas toujours attribuer l'agent correct √† votre requ√™te.

Par cons√©quent, soyez tr√®s explicite sur ce que vous voulez et comment l'IA pourrait proc√©der, par exemple si vous voulez qu'elle effectue une recherche web, ne dites pas:

`Connais-tu de bons pays pour voyager seul ?`

Dites plut√¥t:

`Effectue une recherche web et d√©couvre quels sont les meilleurs pays pour voyager seul`

---

## **Configuration pour ex√©cuter LLM sur votre propre serveur**

Si vous avez un ordinateur puissant ou un serveur auquel vous pouvez acc√©der, mais que vous voulez l'utiliser depuis votre ordinateur portable, vous pouvez choisir d'ex√©cuter le LLM sur un serveur distant en utilisant notre serveur llm personnalis√©.

Sur votre "serveur" qui ex√©cutera le mod√®le d'IA, obtenez l'adresse IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # IP locale
curl https://ipinfo.io/ip # IP publique
```

Note: Pour Windows ou macOS, utilisez ipconfig ou ifconfig pour trouver l'adresse IP.

Clonez le d√©p√¥t et entrez dans le dossier `server/`.

```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Installez les exigences sp√©cifiques au serveur:

```sh
pip3 install -r requirements.txt
```

Ex√©cutez le script du serveur.

```sh
python3 app.py --provider ollama --port 3333
```

Vous pouvez choisir d'utiliser `ollama` et `llamacpp` comme service LLM.

Maintenant sur votre ordinateur personnel:

Changez le fichier `config.ini` pour d√©finir `provider_name` sur `server` et `provider_model` sur `deepseek-r1:xxb`.
D√©finissez `provider_server_address` sur l'adresse IP de la machine qui ex√©cutera le mod√®le.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333
```

Prochaine √©tape: [D√©marrer les services et ex√©cuter AgenticSeek](#d√©marrer-les-services-et-ex√©cuter)  

---

## Parole vers Texte

Avertissement: La speech-to-text ne fonctionne qu'en mode CLI pour le moment.

Notez que la parole vers texte ne fonctionne qu'en anglais pour le moment.

La fonctionnalit√© de parole vers texte est d√©sactiv√©e par d√©faut. Pour l'activer, d√©finissez listen sur True dans le fichier config.ini:

```
listen = True
```

Lorsqu'elle est activ√©e, la fonction de parole vers texte √©coute un mot-cl√© de d√©clenchement, qui est le nom de l'agent, avant de traiter votre entr√©e. Vous pouvez personnaliser le nom de l'agent en mettant √† jour la valeur `agent_name` dans *config.ini*:

```
agent_name = Friday
```

Pour une meilleure reconnaissance, nous recommandons d'utiliser un nom commun en anglais comme "John" ou "Emma" comme nom d'agent.

Une fois que vous voyez la transcription commencer √† appara√Ætre, dites le nom de l'agent √† haute voix pour le r√©veiller (ex: "Friday").

Dites votre requ√™te clairement.

Terminez votre demande par une phrase de confirmation pour indiquer au syst√®me de continuer. Les exemples de phrases de confirmation incluent:
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Configuration

Exemple de configuration:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Exemple Ollama; LM-Studio utilise http://127.0.0.1:1234
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # Liste des langues pour TTS et routage potentiel.
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explication des param√®tres de `config.ini`**:

*   **Section `[MAIN]`:**
    *   `is_local`: `True` si vous utilisez des fournisseurs de LLM locaux (Ollama, LM-Studio, serveur local compatible OpenAI) ou l'option de serveur auto-h√©berg√©. `False` si vous utilisez des API bas√©es sur le cloud (OpenAI, Google, etc.).
    *   `provider_name`: Sp√©cifie le fournisseur de LLM.
        *   Options locales: `ollama`, `lm-studio`, `openai` (pour serveur local compatible OpenAI), `server` (pour configuration de serveur auto-h√©berg√©).
        *   Options d'API: `openai`, `google`, `deepseek`, `huggingface`, `togetherAI`.
    *   `provider_model`: Nom ou ID sp√©cifique du mod√®le du fournisseur s√©lectionn√© (ex: `deepseekcoder:6.7b` pour Ollama, `gpt-3.5-turbo` pour API OpenAI, `mistralai/Mixtral-8x7B-Instruct-v0.1` pour TogetherAI).
    *   `provider_server_address`: L'adresse de votre fournisseur de LLM.
        *   Pour les fournisseurs locaux: ex: `http://127.0.0.1:11434` pour Ollama, `http://127.0.0.1:1234` pour LM-Studio.
        *   Pour le type de fournisseur `server`: L'adresse de votre serveur LLM auto-h√©berg√© (ex: `http://your_server_ip:3333`).
        *   Pour les API cloud (`is_local = False`): Ceci est g√©n√©ralement ignor√© ou peut √™tre laiss√© vide, car les endpoints d'API sont g√©n√©ralement g√©r√©s par les biblioth√®ques clientes.
    *   `agent_name`: Le nom de l'assistant IA (ex: Friday). Si activ√©, utilis√© comme mot de d√©clenchement pour la parole vers texte.
    *   `recover_last_session`: `True` pour tenter de r√©cup√©rer l'√©tat de la session pr√©c√©dente, `False` pour recommencer.
    *   `save_session`: `True` pour sauvegarder l'√©tat de la session actuelle pour une r√©cup√©ration potentielle, `False` sinon.
    *   `speak`: `True` pour activer la sortie vocale de texte vers parole, `False` pour d√©sactiver.
    *   `listen`: `True` pour activer l'entr√©e vocale de parole vers texte (uniquement mode CLI), `False` pour d√©sactiver.
    *   `work_dir`: **Critique:** Le r√©pertoire o√π AgenticSeek lira/√©crira des fichiers. **Assurez-vous que ce chemin est valide et accessible sur votre syst√®me.**
    *   `jarvis_personality`: `True` pour utiliser des invites syst√®me plus "Jarvis-like" (exp√©rimental), `False` pour utiliser des invites standard.
    *   `languages`: Liste de langues s√©par√©es par des virgules (ex: `en, zh, fr`). Utilis√© pour la s√©lection de voix TTS (premi√®re par d√©faut) et peut aider le routeur LLM. Pour √©viter les inefficacit√©s du routeur, √©vitez d'utiliser trop de langues ou des langues tr√®s similaires.
*   **Section `[BROWSER]`:**
    *   `headless_browser`: `True` pour ex√©cuter le navigateur automatis√© sans fen√™tre visible (recommand√© pour l'interface web ou l'utilisation non interactive). `False` pour afficher la fen√™tre du navigateur (utile pour le mode CLI ou le d√©bogage).
    *   `stealth_mode`: `True` pour activer des mesures qui rendent plus difficile la d√©tection de l'automatisation du navigateur. Peut n√©cessiter l'installation manuelle d'extensions de navigateur comme anticaptcha.

Cette section r√©sume les types de fournisseurs de LLM pris en charge. Configurez-les dans `config.ini`.

**Fournisseurs locaux (fonctionnant sur votre propre mat√©riel):**

| Nom du fournisseur dans config.ini | `is_local` | Description                                                                 | Section de configuration                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|------------------------------------------------------------------|
| `ollama`                      | `True`     | Fournit LLM localement facilement en utilisant Ollama.                                             | [Configuration pour ex√©cuter LLM localement sur votre machine](#configuration-pour-ex√©cuter-llm-localement-sur-votre-machine) |
| `lm-studio`                   | `True`     | Fournit LLM localement avec LM-Studio.                                          | [Configuration pour ex√©cuter LLM localement sur votre machine](#configuration-pour-ex√©cuter-llm-localement-sur-votre-machine) |
| `openai` (pour serveur local)   | `True`     | Connectez-vous √† un serveur local exposant une API compatible OpenAI (ex: llama.cpp). | [Configuration pour ex√©cuter LLM localement sur votre machine](#configuration-pour-ex√©cuter-llm-localement-sur-votre-machine) |
| `server`                      | `False`    | Connectez-vous au serveur LLM auto-h√©berg√© d'AgenticSeek fonctionnant sur une autre machine. | [Configuration pour ex√©cuter LLM sur votre propre serveur](#configuration-pour-ex√©cuter-llm-sur-votre-propre-serveur) |

**Fournisseurs d'API (bas√©s sur le cloud):**

| Nom du fournisseur dans config.ini | `is_local` | Description                                      | Section de configuration                                       |
|-------------------------------|------------|--------------------------------------------------|-----------------------------------------------------|
| `openai`                      | `False`    | Utilise l'API officielle d'OpenAI (ex: GPT-3.5, GPT-4). | [Configuration pour ex√©cuter avec une API](#configuration-pour-ex√©cuter-avec-une-api) |
| `google`                      | `False`    | Utilise les mod√®les Google Gemini via API.              | [Configuration pour ex√©cuter avec une API](#configuration-pour-ex√©cuter-avec-une-api) |
| `deepseek`                    | `False`    | Utilise l'API officielle de Deepseek.                     | [Configuration pour ex√©cuter avec une API](#configuration-pour-ex√©cuter-avec-une-api) |
| `huggingface`                 | `False`    | Utilise Hugging Face Inference API.                  | [Configuration pour ex√©cuter avec une API](#configuration-pour-ex√©cuter-avec-une-api) |
| `togetherAI`                  | `False`    | Utilise divers mod√®les ouverts via l'API TogetherAI.    | [Configuration pour ex√©cuter avec une API](#configuration-pour-ex√©cuter-avec-une-api) |

---
## D√©pannage

Si vous rencontrez des probl√®mes, cette section fournit des conseils.

# Probl√®mes connus

## Probl√®mes de ChromeDriver

**Exemple d'erreur:** `SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX`

### Cause racine
L'incompatibilit√© de version de ChromeDriver se produit lorsque:
1. La version de ChromeDriver que vous avez install√©e ne correspond pas √† la version du navigateur Chrome
2. Dans les environnements Docker, `undetected_chromedriver` peut t√©l√©charger sa propre version de ChromeDriver, contournant les binaires mont√©s

### √âtapes de r√©solution

#### 1. V√©rifiez votre version de Chrome
Ouvrez Google Chrome ‚Üí `Param√®tres > √Ä propos de Chrome` pour trouver votre version (ex: "Version 134.0.6998.88")

#### 2. T√©l√©chargez ChromeDriver correspondant

**Pour Chrome 115 et versions ult√©rieures:** Utilisez [Chrome for Testing API](https://googlechromelabs.github.io/chrome-for-testing/)
- Visitez le tableau de disponibilit√© de Chrome for Testing
- Trouvez votre version de Chrome ou la correspondance disponible la plus proche
- T√©l√©chargez ChromeDriver pour votre syst√®me d'exploitation (utilisez Linux64 pour les environnements Docker)

**Pour les anciennes versions de Chrome:** Utilisez [T√©l√©chargements h√©rit√©s de ChromeDriver](https://chromedriver.chromium.org/downloads)

![T√©l√©charger ChromeDriver depuis Chrome for Testing](./media/chromedriver_readme.png)

#### 3. Installez ChromeDriver (choisissez une m√©thode)

**M√©thode A: R√©pertoire racine du projet (recommand√© pour Docker)**
```bash
# Placez le binaire chromedriver t√©l√©charg√© dans le r√©pertoire racine du projet
cp path/to/downloaded/chromedriver ./chromedriver
chmod +x ./chromedriver  # Rendez-le ex√©cutable sur Linux/macOS
```

**M√©thode B: PATH syst√®me**
```bash
# Linux/macOS
sudo mv chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver

# Windows: Placez chromedriver.exe dans un dossier du PATH
```

#### 4. V√©rifiez l'installation
```bash
# Testez la version de ChromeDriver
./chromedriver --version
# Ou s'il est dans PATH:
chromedriver --version
```

### Instructions sp√©cifiques √† Docker

‚ö†Ô∏è **Important pour les utilisateurs de Docker:**
- La m√©thode de montage de volumes Docker peut ne pas fonctionner avec le mode furtif (`undetected_chromedriver`)
- **Solution:** Placez ChromeDriver dans le r√©pertoire racine du projet en tant que `./chromedriver`
- L'application le d√©tectera automatiquement et utilisera ce binaire
- Vous devriez voir dans les logs: `"Using ChromeDriver from project root: ./chromedriver"`

### Conseils de d√©pannage

1. **Toujours une incompatibilit√© de version ?**
   - V√©rifiez que ChromeDriver est ex√©cutable: `ls -la ./chromedriver`
   - V√©rifiez la version de ChromeDriver: `./chromedriver --version`
   - Assurez-vous qu'elle correspond √† votre version du navigateur Chrome

2. **Probl√®mes avec le conteneur Docker ?**
   - V√©rifiez les logs du backend: `docker logs backend`
   - Recherchez le message: `"Using ChromeDriver from project root"`
   - S'il n'est pas trouv√©, v√©rifiez que le fichier existe et est ex√©cutable

3. **Versions de Chrome for Testing**
   - Utilisez une correspondance exacte lorsque possible
   - Pour la version 134.0.6998.88, utilisez ChromeDriver 134.0.6998.165 (la version disponible la plus proche)
   - Le num√©ro de version principal doit correspondre (134 = 134)

### Matrice de compatibilit√© des versions

| Version de Chrome | Version de ChromeDriver | Statut |
|----------------|---------------------|---------|
| 134.0.6998.x   | 134.0.6998.165     | ‚úÖ Disponible |
| 133.0.6943.x   | 133.0.6943.141     | ‚úÖ Disponible |
| 132.0.6834.x   | 132.0.6834.159     | ‚úÖ Disponible |

*Pour la compatibilit√© la plus r√©cente, consultez le [Tableau de Chrome for Testing](https://googlechromelabs.github.io/chrome-for-testing/)*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Cela se produit si votre navigateur et la version de chromedriver ne correspondent pas.

Vous devez naviguer pour t√©l√©charger la derni√®re version:

https://developer.chrome.com/docs/chromedriver/downloads

Si vous utilisez Chrome version 115 ou sup√©rieure, allez √†:

https://googlechromelabs.github.io/chrome-for-testing/

et t√©l√©chargez la version de chromedriver correspondant √† votre syst√®me d'exploitation.

![alt text](./media/chromedriver_readme.png)

Si cette section est incompl√®te, ouvrez un issue.

##  Probl√®mes d'adaptateurs de connexion

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (note: le port peut varier)
```

*   **Cause:** Il manque le pr√©fixe `http://` dans `provider_server_address` pour `lm-studio` (ou un autre serveur local compatible OpenAI similaire) dans `config.ini`, ou il pointe vers le mauvais port.
*   **Solution:**
    *   Assurez-vous que l'adresse inclut `http://`. LM-Studio utilise g√©n√©ralement `http://127.0.0.1:1234` par d√©faut.
    *   `config.ini` correct: `provider_server_address = http://127.0.0.1:1234` (ou votre port r√©el du serveur LM-Studio).

## URL de base de SearxNG non fournie

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
```

Cela peut se produire si vous ex√©cutez le mode CLI avec une URL de base de searxng incorrecte.

SEARXNG_BASE_URL doit diff√©rer selon que vous ex√©cutez dans Docker ou sur l'h√¥te:

**Ex√©cution sur l'h√¥te:** `SEARXNG_BASE_URL="http://localhost:8080"`

**Ex√©cution compl√®tement dans Docker (interface web):** `SEARXNG_BASE_URL="http://searxng:8080"`

## FAQ

**Q: De quel mat√©riel ai-je besoin ?**  

| Taille du mod√®le  | GPU  | Commentaires                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB VRAM | ‚ö†Ô∏è Non recommand√©. Performances m√©diocres, hallucinations fr√©quentes, les agents de planification peuvent √©chouer. |
| 14B        | 12 GB VRAM (ex: RTX 3060) | ‚úÖ Utilisable pour des t√¢ches simples. Peut avoir des difficult√©s avec la navigation web et la planification de t√¢ches. |
| 32B        | 24+ GB VRAM (ex: RTX 4090) | üöÄ R√©ussit la plupart des t√¢ches, peut encore avoir des difficult√©s avec la planification de t√¢ches |
| 70B+        | 48+ GB VRAM | üí™ Excellent. Recommand√© pour les cas d'utilisation avanc√©s. |

**Q: Que faire si je rencontre des erreurs ?**  

Assurez-vous que le local fonctionne (`ollama serve`), que votre `config.ini` correspond √† votre fournisseur et que les d√©pendances sont install√©es. Si rien ne fonctionne, n'h√©sitez pas √† ouvrir un issue.

**Q: Peut-il vraiment fonctionner √† 100% localement ?**  

Oui, avec les fournisseurs Ollama, lm-studio ou server, tous les mod√®les de parole vers texte, LLM et texte vers parole fonctionnent localement. Les options non locales (OpenAI ou autres API) sont optionnelles.

**Q: Pourquoi devrais-je utiliser AgenticSeek quand j'ai Manus ?**

Contrairement √† Manus, AgenticSeek privil√©gie l'ind√©pendance des syst√®mes externes, vous donnant plus de contr√¥le, de confidentialit√© et √©vitant les co√ªts d'API.

**Q: Qui est derri√®re ce projet ?**

Ce projet a √©t√© cr√©√© par moi, avec deux amis comme mainteneurs et des contributeurs de la communaut√© open source sur GitHub. Nous sommes juste des individus passionn√©s, pas une startup, ni affili√©s √† aucune organisation.

Tout compte AgenticSeek sur X autre que mon compte personnel (https://x.com/Martin993886460) est un imposteur.

## Contribuer

Nous recherchons des d√©veloppeurs pour am√©liorer AgenticSeek ! Consultez les probl√®mes ouverts ou les discussions.

[Guide de contribution](./docs/CONTRIBUTING.md)

## Sponsors:

Vous voulez am√©liorer les capacit√©s d'AgenticSeek avec des fonctionnalit√©s comme la recherche de vols, la planification de voyages ou l'obtention des meilleures offres d'achat ? Envisagez d'utiliser SerpApi pour cr√©er des outils personnalis√©s qui d√©bloquent plus de fonctionnalit√©s de type Jarvis. Avec SerpApi, vous pouvez acc√©l√©rer votre agent pour des t√¢ches professionnelles tout en gardant le contr√¥le total.

<a href="https://serpapi.com/"><img src="./media/banners/sponsor_banner_serpapi.png" height="350" alt="SerpApi Banner" ></a>

Consultez [Contributing.md](./docs/CONTRIBUTING.md) pour apprendre comment int√©grer des outils personnalis√©s !

### **Sponsors**:

- [tatra-labs](https://github.com/tatra-labs)

## Mainteneurs:

 > [Fosowl](https://github.com/Fosowl) | Heure de Paris 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Heure de Taipei 

## Remerciements sp√©ciaux:

 > [tcsenpai](https://github.com/tcsenpai) et [plitc](https://github.com/plitc) pour avoir aid√© √† la dockerisation du backend

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)
